{"input":"import { generateFallbackWaveform } from '../../shared/fallbackAudio.js';\nimport { jsonResponse, readJsonBody, sanitizeText } from '../lib/utils.js';\nimport { getUserFromRequest } from '../lib/auth.js';\nimport { enforceApiCallLimit } from '../lib/apiUsage.js';\nimport { getSubscriptionContext } from '../lib/entitlements.js';\nimport { getTtsLimits, enforceTtsRateLimit } from '../lib/ttsLimits.js';\nimport { resolveEnv } from '../lib/environment.js';\nimport { EMOTION_INSTRUCTIONS } from '../lib/emotionInstructions.js';\n\nfunction normalizeAzureEndpoint(rawEndpoint) {\n  return String(rawEndpoint || '')\n    .replace(/\\/+$/, '')\n    .replace(/\\/openai\\/v1\\/?$/, '')\n    .replace(/\\/openai\\/?$/, '');\n}\n\n/**\n * Cloudflare Pages Function that provides text-to-speech audio as a data URI or streaming response.\n *\n * Enhanced with gpt-4o-mini-tts steerable instructions for context-aware,\n * mystical tarot reading narration.\n *\n * Supports:\n * - Context-specific instruction templates (card-reveal, full-reading, synthesis)\n * - Voice selection (verse, nova, shimmer, alloy, echo, fable, onyx, etc.)\n * - Speed control for contemplative pacing (0.25-4.0, default 1.1)\n * - Streaming mode for real-time audio playback\n * - Graceful fallback to local waveform\n *\n * Usage:\n *\n * Non-streaming mode (returns JSON with base64 data URI):\n *   POST /api/tts\n *   Body: { \"text\": \"...\", \"context\": \"full-reading\", \"voice\": \"verse\", \"speed\": 0.9 }\n *   Response: { \"audio\": \"data:audio/mp3;base64,...\", \"provider\": \"azure-gpt-4o-mini-tts\" }\n *\n * Streaming mode (returns audio stream):\n *   POST /api/tts?stream=true\n *   Body: { \"text\": \"...\", \"context\": \"full-reading\", \"voice\": \"verse\", \"speed\": 0.9 }\n *   Response: audio/mp3 stream (content-type: audio/mp3)\n *\n * API Reference: https://learn.microsoft.com/en-us/azure/ai-foundry/openai/reference-preview-latest#create-speech\n */\nexport const onRequestGet = async ({ env }) => {\n  // Health check endpoint\n  // TTS can use separate credentials (AZURE_OPENAI_TTS_*) or fall back to shared credentials\n  const azureEndpoint = resolveEnv(env, 'AZURE_OPENAI_TTS_ENDPOINT') || resolveEnv(env, 'AZURE_OPENAI_ENDPOINT');\n  const azureKey = resolveEnv(env, 'AZURE_OPENAI_TTS_API_KEY') || resolveEnv(env, 'AZURE_OPENAI_API_KEY');\n  const azureDeployment = resolveEnv(env, 'AZURE_OPENAI_GPT_AUDIO_MINI_DEPLOYMENT');\n  const speechRouting = resolveSpeechRouting({\n    apiVersion: resolveEnv(env, 'AZURE_OPENAI_API_VERSION'),\n    useV1Format: resolveEnv(env, 'AZURE_OPENAI_USE_V1_FORMAT')\n  });\n  const audioFormat = resolveEnv(env, 'AZURE_OPENAI_GPT_AUDIO_MINI_FORMAT') || 'mp3';\n  const hasAzure = !!(azureEndpoint && azureKey && azureDeployment);\n  return jsonResponse({\n    status: 'ok',\n    provider: hasAzure ? 'azure-openai' : 'local',\n    apiVersion: speechRouting.apiVersion,\n    useV1Format: speechRouting.useV1Format,\n    format: audioFormat,\n    timestamp: new Date().toISOString()\n  });\n};\n\nexport const onRequestPost = async ({ request, env }) => {\n  const requestId = crypto.randomUUID();\n  try {\n    const url = new URL(request.url);\n    const stream = url.searchParams.get('stream') === 'true';\n\n    // Get user and subscription info\n    const user = await getUserFromRequest(request, env);\n    const subscription = getSubscriptionContext(user);\n    const tier = subscription.tier;\n    const effectiveTier = subscription.effectiveTier;\n    const ttsLimits = getTtsLimits(effectiveTier);\n\n    const { text, context, voice, speed, format, response_format: responseFormat, emotion } = await readJsonBody(request);\n    const sanitizedText = sanitizeText(text, { maxLength: 4096, collapseWhitespace: false });\n    const requestedFormat = normalizeSpeechFormat(format ?? responseFormat);\n    const debugLoggingEnabled = isTtsDebugLoggingEnabled(env);\n\n    if (!sanitizedText) {\n      return jsonResponse(\n        { error: 'The \"text\" field is required.' },","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"functions__api__tts__part-1.mp3"}
{"input":"        { status: 400 }\n      );\n    }\n\n    // API key usage is Pro-only and subject to API call limits.\n    if (user?.auth_provider === 'api_key') {\n      const apiLimit = await enforceApiCallLimit(env, user);\n      if (!apiLimit.allowed) {\n        return jsonResponse(apiLimit.payload, { status: apiLimit.status });\n      }\n    }\n\n    // Check tier-based rate limits (in addition to global rate limit)\n    const rateLimitResult = await enforceTtsRateLimit(env, request, user, ttsLimits, requestId);\n    if (rateLimitResult?.limited) {\n      const errorCode = rateLimitResult.tierLimited ? 'TIER_LIMIT' : 'RATE_LIMIT';\n      const errorMessage = rateLimitResult.tierLimited\n        ? `You've reached your monthly TTS limit (${ttsLimits.monthly}). Upgrade to Plus or Pro for more.`\n        : 'Too many text-to-speech requests. Please wait a few moments and try again.';\n\n      return jsonResponse(\n        {\n          error: errorMessage,\n          errorCode,\n          tierLimited: rateLimitResult.tierLimited || false,\n          currentTier: tier,\n          limit: rateLimitResult.limit ?? null,\n          used: rateLimitResult.used ?? null,\n          resetAt: rateLimitResult.resetAt ?? null\n        },\n        {\n          status: 429,\n          headers: {\n            'retry-after': rateLimitResult.retryAfter.toString()\n          }\n        }\n      );\n    }\n\n    // Primary: Azure OpenAI gpt-4o-mini-tts with steerable instructions\n    // TTS can use separate credentials (AZURE_OPENAI_TTS_*) or fall back to shared credentials\n    const azureConfig = {\n      endpoint: normalizeAzureEndpoint(resolveEnv(env, 'AZURE_OPENAI_TTS_ENDPOINT') || resolveEnv(env, 'AZURE_OPENAI_ENDPOINT')),\n      apiKey: resolveEnv(env, 'AZURE_OPENAI_TTS_API_KEY') || resolveEnv(env, 'AZURE_OPENAI_API_KEY'),\n      deployment: resolveEnv(env, 'AZURE_OPENAI_GPT_AUDIO_MINI_DEPLOYMENT'),\n      apiVersion: resolveEnv(env, 'AZURE_OPENAI_API_VERSION'),\n      format: requestedFormat || resolveEnv(env, 'AZURE_OPENAI_GPT_AUDIO_MINI_FORMAT'),\n      useV1Format: resolveEnv(env, 'AZURE_OPENAI_USE_V1_FORMAT'),\n      debugLoggingEnabled\n    };\n\n    if (azureConfig.endpoint && azureConfig.apiKey && azureConfig.deployment) {\n      try {\n        if (stream) {\n          // Return streaming response\n          return await generateWithAzureGptMiniTTSStream(azureConfig, {\n            text: sanitizedText,\n            context: context || 'default',\n            voice: voice || 'verse',\n            speed: speed,\n            emotion: emotion || null\n          });\n        } else {\n          // Return complete audio as data URI via /audio/speech\n          const audio = await generateWithAzureGptMiniTTS(azureConfig, {\n            text: sanitizedText,\n            context: context || 'default',\n            voice: voice || 'verse',\n            speed: speed,\n            emotion: emotion || null\n          });\n          if (audio) {\n            return jsonResponse({ audio, provider: 'azure-gpt-4o-mini-tts' });\n          }\n        }\n      } catch (error) {\n        console.error(`[${requestId}] [tts] Azure gpt-4o-mini-tts failed, falling back to local waveform:`, error);\n      }\n    }\n\n    // Fallback: local synthesized waveform (no external dependency).\n    const fallbackAudio = generateFallbackWaveform(sanitizedText);\n\n    if (stream) {\n      // For streaming requests, decode base64 and return binary audio\n      const base64Data = fallbackAudio.split(',')[1];\n      const binaryString = atob(base64Data);\n      const bytes = new Uint8Array(binaryString.length);\n      for (let i = 0; i < binaryString.length; i++) {\n        bytes[i] = binaryString.charCodeAt(i);\n      }\n      return new Response(bytes, {\n        headers: {\n          'content-type': 'audio/wav',\n          'cache-control': 'no-cache',\n          'x-tts-provider': 'fallback'\n        }\n      });\n    }\n","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"functions__api__tts__part-2.mp3"}
{"input":"    return jsonResponse({ audio: fallbackAudio, provider: 'fallback' });\n  } catch (error) {\n    // Return 400 for JSON parse errors (client mistake)\n    if (error?.message === 'Invalid JSON payload.') {\n      return jsonResponse({ error: 'Invalid JSON payload.' }, { status: 400 });\n    }\n    console.error(`[${requestId}] [tts] Function error:`, error);\n    return jsonResponse(\n      { error: 'Unable to generate audio at this time.' },\n      { status: 500 }\n    );\n  }\n};\n\n// sanitizeText is now imported from ../lib/utils.js\n\n/**\n * Convert Uint8Array to base64 string.\n * Used for encoding audio binary data into data URIs.\n */\nfunction uint8ToBase64(uint8Array) {\n  let binary = '';\n  for (let i = 0; i < uint8Array.length; i++) {\n    binary += String.fromCharCode(uint8Array[i]);\n  }\n  return btoa(binary);\n}\n\n/**\n * Steerable instruction templates for different tarot reading contexts.\n * These leverage gpt-4o-mini-tts's ability to control tone, pacing, and delivery style.\n */\nconst INSTRUCTION_TEMPLATES = {\n  'card-reveal': `Speak gently and mystically, as a tarot reader revealing a single card with reverence.\n    Use a slightly slower pace with brief pauses after the card name and orientation.\n    Convey wisdom and contemplation in your tone.`,\n\n  'full-reading': `Speak as a wise, compassionate tarot reader sharing a complete reading.\n    Use a thoughtful, contemplative tone with natural pauses between card descriptions and themes.\n    Allow space for reflectionâ€”speak slowly and deliberately, as if sitting across from the querent.\n    Convey mystical depth while remaining grounded and accessible.\n    Maintain a gentle, trauma-informed presence throughout.`,\n\n  'synthesis': `Speak as a tarot reader weaving together the threads of a reading into cohesive guidance.\n    Use a flowing, storytelling cadence that connects themes and patterns.\n    Pause briefly between major insights to allow integration.\n    Convey both wisdom and warmth, emphasizing agency and empowerment.`,\n\n  'question': `Speak gently and clearly, acknowledging the querent's question with respect.\n    Use a warm, inviting tone that creates space for exploration rather than fixed answers.`,\n\n  'reflection': `Speak softly and affirmingly, honoring the querent's personal reflections.\n    Use a validating, supportive tone that acknowledges their intuitive insights.`,\n\n  'default': `Speak thoughtfully and gently, as a tarot reader sharing wisdom.\n    Use a mystical yet grounded tone with natural pacing and slight pauses for contemplation.`\n};\n\nfunction normalizeSpeechFormat(value) {\n  if (typeof value !== 'string') return null;\n  const trimmed = value.trim();\n  return trimmed.length ? trimmed.toLowerCase() : null;\n}\n\nfunction resolveUseV1Format(env) {\n  if (!env) return false;\n  if (typeof env.useV1Format !== 'undefined' && env.useV1Format !== null && env.useV1Format !== '') {\n    return parseBooleanFlag(env.useV1Format);\n  }\n  const apiVersion = String(env.apiVersion || '').trim().toLowerCase();\n  return apiVersion === 'preview' || apiVersion === 'v1';\n}\n\nfunction resolveSpeechApiVersion(env, useV1Format) {\n  const rawApiVersion = String(env?.apiVersion || '').trim();\n  const normalized = rawApiVersion.toLowerCase();\n  if (useV1Format) {\n    if (normalized === 'preview' || normalized === 'v1') {\n      return normalized;\n    }\n    return 'preview';\n  }\n  return rawApiVersion || '2025-04-01-preview';\n}\n\nfunction resolveSpeechRouting(env) {\n  const useV1Format = resolveUseV1Format(env);\n  const apiVersion = resolveSpeechApiVersion(env, useV1Format);\n  return { useV1Format, apiVersion };\n}\n\nfunction buildSpeechFormData(payload) {\n  const formData = new FormData();\n  for (const [key, value] of Object.entries(payload)) {\n    if (typeof value === 'undefined' || value === null) continue;\n    formData.append(key, typeof value === 'string' ? value : String(value));\n  }","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"functions__api__tts__part-3.mp3"}
{"input":"  return formData;\n}\n\nfunction buildSpeechRequestOptions(env, payload, mode = 'multipart') {\n  const headers = {\n    'api-key': env.apiKey,\n    'accept': 'application/octet-stream'\n  };\n\n  if (mode === 'json') {\n    headers['content-type'] = 'application/json';\n    return { headers, body: JSON.stringify(payload), mode };\n  }\n\n  return { headers, body: buildSpeechFormData(payload), mode };\n}\n\nfunction shouldRetrySpeechAsJson(status, errorText) {\n  if (status === 415) return true;\n  if (status !== 400) return false;\n  const normalized = (errorText || '').toLowerCase();\n  return normalized.includes('content-type') || normalized.includes('multipart') || normalized.includes('form');\n}\n\nasync function fetchSpeechWithFallback(url, env, payload, debugLoggingEnabled, useV1Format = false) {\n  // v1 preview format expects JSON; legacy deployment format may need multipart\n  const attempts = useV1Format ? ['json', 'multipart'] : ['multipart', 'json'];\n  let lastError = null;\n\n  for (const mode of attempts) {\n    if (debugLoggingEnabled) {\n      console.log(`[TTS] Request mode: ${mode}`);\n    }\n\n    const { headers, body } = buildSpeechRequestOptions(env, payload, mode);\n    const response = await fetch(url, {\n      method: 'POST',\n      headers,\n      body\n    });\n\n    if (response.ok) {\n      return { response, mode };\n    }\n\n    const errText = await response.text().catch(() => '');\n    const preview = errText.slice(0, 1000);\n\n    if (debugLoggingEnabled && preview) {\n      console.warn(`[TTS] ${mode} error response:`, preview);\n    }\n\n    if (mode === 'multipart' && shouldRetrySpeechAsJson(response.status, preview)) {\n      lastError = new Error(`Azure TTS error ${response.status}: ${preview}`);\n      continue;\n    }\n\n    throw new Error(`Azure TTS error ${response.status}: ${preview}`);\n  }\n\n  if (lastError) throw lastError;\n  throw new Error('Azure TTS request failed without a response.');\n}\n\n/**\n * Build TTS request configuration shared by both streaming and non-streaming modes.\n * Extracts common logic for endpoint construction, payload building, and parameter validation.\n *\n * @param {Object} env - Environment configuration\n * @param {Object} options - TTS options\n * @param {string} options.text - Text to synthesize\n * @param {string} options.context - Context template (card-reveal, full-reading, etc.)\n * @param {string} [options.voice='verse'] - Voice selection\n * @param {number} [options.speed=1.1] - Speech speed (0.25-4.0)\n * @param {string} [options.emotion=null] - Emotion from GraphRAG analysis\n * @returns {Object} Request configuration with url, payload, format, etc.\n */\nfunction buildTTSRequest(env, { text, context, voice, speed, emotion }) {\n  const endpoint = normalizeAzureEndpoint(env.endpoint);\n  const deployment = env.deployment;\n  const format = normalizeSpeechFormat(env.format) || 'mp3';\n  const debugLoggingEnabled = Boolean(env.debugLoggingEnabled);\n\n  // API version logic:\n  // - v1 format uses \"preview\" or \"v1\"\n  // - deployment format uses dated preview version (e.g., \"2025-04-01-preview\")\n  const { useV1Format, apiVersion } = resolveSpeechRouting(env);\n\n  // Select instruction template based on context\n  let instructions = INSTRUCTION_TEMPLATES[context] || INSTRUCTION_TEMPLATES.default;\n\n  // Merge emotion-specific instructions if provided\n  if (emotion && EMOTION_INSTRUCTIONS[emotion]) {\n    instructions = `${instructions}\\n\\nEmotional tone for this reading: ${EMOTION_INSTRUCTIONS[emotion]}`;\n  }\n\n  // Voice validation (gpt-4o-mini-tts voices - 11 available)\n  // Base voices: alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, shimmer, verse\n  const validVoices = ['alloy', 'ash', 'ballad', 'coral', 'echo', 'fable', 'nova', 'onyx', 'sage', 'shimmer', 'verse'];\n  const selectedVoice = validVoices.includes(voice) ? voice : 'verse';\n\n  // Speed validation (0.25 - 4.0 range per API spec)","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"functions__api__tts__part-4.mp3"}
{"input":"  // Coerce to number first to handle string/invalid inputs\n  const parsedSpeed = speed !== undefined ? parseFloat(speed) : NaN;\n  const selectedSpeed = Number.isFinite(parsedSpeed)\n    ? Math.max(0.25, Math.min(4.0, parsedSpeed))\n    : 1.1; // Default: slightly faster for engaging tarot reading pace\n\n  // Build URL based on format preference\n  const url = useV1Format\n    ? `${endpoint}/openai/v1/audio/speech?api-version=${apiVersion}`\n    : `${endpoint}/openai/deployments/${deployment}/audio/speech?api-version=${apiVersion}`;\n\n  // Build payload per API specification\n  const payload = {\n    input: text,\n    model: deployment,\n    voice: selectedVoice,\n    response_format: format,\n    speed: selectedSpeed\n  };\n\n  // Check if this deployment supports steerable instructions\n  const isSteerableModel = /gpt-4o|mini-tts|audio-preview/i.test(deployment);\n\n  if (isSteerableModel) {\n    payload.instructions = instructions;\n  }\n\n  return { url, payload, format, useV1Format, apiVersion, debugLoggingEnabled };\n}\n\n/**\n * Enhanced Azure OpenAI TTS generation with optional steerable instructions.\n *\n * For steerable-capable models (e.g. gpt-4o-mini-tts, audio-preview variants), includes\n * context-aware instructions. For standard models (tts-1, tts-1-hd), omits unsupported fields.\n * This keeps behavior model-agnostic while preserving rich narration when available.\n *\n * API Reference: https://learn.microsoft.com/en-us/azure/ai-foundry/openai/reference-preview-latest#create-speech\n */\nasync function generateWithAzureGptMiniTTS(env, { text, context, voice, speed, emotion }) {\n  const { url, payload, format, useV1Format, debugLoggingEnabled } = buildTTSRequest(env, { text, context, voice, speed, emotion });\n\n  if (debugLoggingEnabled) {\n    console.log('[TTS] Request URL:', url);\n    console.log('[TTS] Request payload:', JSON.stringify(payload, null, 2));\n  }\n\n  const { response } = await fetchSpeechWithFallback(url, env, payload, debugLoggingEnabled, useV1Format);\n\n  if (debugLoggingEnabled) {\n    console.log('[TTS] Response status:', response.status, response.statusText);\n    console.log('[TTS] Response headers:', JSON.stringify([...response.headers.entries()]));\n  }\n\n  const arrayBuffer = await response.arrayBuffer();\n  const base64 = uint8ToBase64(new Uint8Array(arrayBuffer));\n  // Use standard MIME types: audio/mpeg for MP3 (Safari rejects audio/mp3)\n  const mime = format === 'wav' ? 'audio/wav' : format === 'mp3' ? 'audio/mpeg' : `audio/${format}`;\n  return `data:${mime};base64,${base64}`;\n}\n\n/**\n * Streaming Azure OpenAI TTS generation.\n *\n * Uses the stream_format parameter to request Server-Sent Events (SSE)\n * or raw audio streaming from Azure OpenAI.\n *\n * Per API docs: \"sse is not supported for tts-1 or tts-1-hd\"\n * Use stream_format: 'audio' for all models\n *\n * API Reference: https://learn.microsoft.com/en-us/azure/ai-foundry/openai/reference-preview-latest#create-speech\n */\nasync function generateWithAzureGptMiniTTSStream(env, { text, context, voice, speed, emotion }) {\n  const { url, payload, format, useV1Format, debugLoggingEnabled } = buildTTSRequest(env, { text, context, voice, speed, emotion });\n\n  // Add streaming parameter\n  payload.stream_format = 'audio'; // Stream raw audio chunks (safer, works with all models)\n\n  if (debugLoggingEnabled) {\n    console.log('[TTS Streaming] Request URL:', url);\n    console.log('[TTS Streaming] Request payload:', JSON.stringify(payload, null, 2));\n  }\n\n  const { response } = await fetchSpeechWithFallback(url, env, payload, debugLoggingEnabled, useV1Format);\n\n  if (debugLoggingEnabled) {\n    console.log('[TTS Streaming] Response status:', response.status, response.statusText);\n  }\n\n  // Return the streaming response directly\n  // The response body is a ReadableStream of audio chunks\n  // Use standard MIME types: audio/mpeg for MP3 (Safari rejects audio/mp3)","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"functions__api__tts__part-5.mp3"}
{"input":"  const mime = format === 'wav' ? 'audio/wav' : format === 'mp3' ? 'audio/mpeg' : `audio/${format}`;\n  return new Response(response.body, {\n    headers: {\n      'content-type': mime,\n      'cache-control': 'no-cache',\n      'x-tts-provider': 'azure-gpt-4o-mini-tts'\n    }\n  });\n}\n\nfunction parseBooleanFlag(value) {\n  if (typeof value === 'boolean') {\n    return value;\n  }\n\n  if (typeof value === 'number') {\n    return value !== 0;\n  }\n\n  if (typeof value === 'string') {\n    const normalized = value.trim().toLowerCase();\n    return normalized === 'true' || normalized === '1' || normalized === 'yes';\n  }\n\n  return false;\n}\n\nfunction isTtsDebugLoggingEnabled(env) {\n  const explicit = resolveEnv(env, 'ENABLE_TTS_DEBUG_LOGGING');\n  if (typeof explicit !== 'undefined') {\n    return parseBooleanFlag(explicit);\n  }\n\n  const nodeEnv = resolveEnv(env, 'NODE_ENV');\n  if (nodeEnv && nodeEnv.toLowerCase() !== 'production') {\n    return true;\n  }\n\n  const mode = resolveEnv(env, 'MODE');\n  if (mode && mode.toLowerCase() !== 'production') {\n    return true;\n  }\n\n  return false;\n}\n","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"functions__api__tts__part-6.mp3"}
{"input":"import { normalizeReadingText, prepareForTTS } from './formatting.js';\nimport { generateFallbackWaveform } from '../../shared/fallbackAudio.js';\nimport { djb2Hash } from './utils.js';\nimport { safeStorage } from './safeStorage.js';\n\nlet flipAudio = null;\nlet ambienceAudio = null;\nconst AMBIENCE_BASE_VOLUME = 0.2;\nlet ambienceSwellTimer = null;\nlet ambienceVolumeRaf = null;\nlet ttsAudio = null;\nlet currentTTSState = {\n  status: 'idle',\n  provider: null,\n  source: null,\n  cached: false,\n  error: null,\n  errorCode: null,\n  errorDetails: null,\n  message: null,\n  reason: null,\n  context: null,\n  // Progress tracking\n  currentTime: 0,\n  duration: 0,\n  progress: 0\n};\nconst ttsListeners = new Set();\nlet currentNarrationRequestId = 0;\nlet activeNarrationId = null;\nlet cancelledUpToRequestId = 0;\nlet ttsAbortController = null;\nlet audioUnlocked = false;\nlet unlockListenersRegistered = false;\nconst TTS_CACHE_PREFIX = 'tts_cache_';\nconst TTS_CACHE_MAX_ENTRIES = 50;\nconst TTS_CACHE_PURGE_THRESHOLD = 30;\nconst TTS_CACHE_PURGE_TARGET = 20;\nconst TTS_CACHE_EVICT_BATCH = 10;\nconst TTS_CACHE_SWEEP_INTERVAL_MS = 5 * 60 * 1000;\nconst CACHE_MAX_AGE_MS = 7 * 24 * 60 * 60 * 1000;\nconst trackedObjectUrls = new Set();\nconst audioObjectUrlMap = new WeakMap();\nlet cleanupListenersRegistered = false;\nlet lastCacheSweep = 0;\nconst ttsStreamQueue = [];\nlet ttsStreamActive = false;\nlet ttsStreamFinalized = false;\nlet ttsStreamProcessing = false;\nlet ttsStreamRequestId = 0;\nlet ttsStreamAbortController = null;\n\nconst SILENT_AUDIO_URI = 'data:audio/mp3;base64,//MkxAAHiAICWABElBeKPL/RANb2w+yiT1g/gTok//lP/W/l3h8QO/OCdCqCW2Cw//MkxAQHkAIWUAhEmAQXWUOFW2dxPu//9mr60ElY5sseQ+xxesmHKtZr7bsqqX2L//MkxAgFwAYiQAhEAC2hq22d3///9FTV6tA36JdgBJoOGgc+7qvqej5Zu7/7uI9l//MkxBQHAAYi8AhEAO193vt9KGOq+6qcT7hhfN5FTInmwk8RkqKImTM55pRQHQSq//MkxBsGkgoIAABHhTACIJLf99nVI///yuW1uBqWfEu7CgNPWGpUadBmZ////4sL//MkxCMHMAH9iABEmAsKioqKigsLCwtVTEFNRTMuOTkuNVVVVVVVVVVVVVVVVVVV//MkxCkECAUYCAAAAFVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV';\n\n/**\n * Unlock audio playback by creating and playing a silent audio element.\n * Must be called from a user interaction event (click, touch, keydown, etc.)\n * to satisfy browser autoplay policies.\n */\nexport async function unlockAudio() {\n  if (audioUnlocked) return true;\n  if (typeof Audio === 'undefined' || typeof window === 'undefined') return false;\n\n  ensureGlobalCleanupListeners();\n\n  try {\n    const silentAudio = new Audio(SILENT_AUDIO_URI);\n    silentAudio.volume = 0.01;\n    await silentAudio.play();\n    audioUnlocked = true;\n    return true;\n  } catch (err) {\n    console.warn('[Audio] Audio unlock failed - user interaction needed', err);\n    emitTTSState({\n      status: 'unlock-failed',\n      reason: 'interaction-required',\n      error: err?.message || String(err),\n      message: 'Tap anywhere on the page to enable audio.'\n    });\n    return false;\n  }\n}\n\nexport function initAudio() {\n  ensureGlobalCleanupListeners();\n\n  if (typeof Audio === 'undefined') {\n    return {\n      flipAudio: null,\n      ambienceAudio: null\n    };\n  }\n\n  // Set up robust unlock listeners on first init\n  if (typeof window !== 'undefined' && !unlockListenersRegistered) {\n    unlockListenersRegistered = true;\n    const unlockEvents = ['click', 'touchstart', 'keydown'];\n\n    const unlockHandler = () => {\n      // Create and play immediately within the event handler\n      const s = new Audio(SILENT_AUDIO_URI);\n      s.volume = 0.01;\n      const p = s.play();\n\n      if (p !== undefined) {\n        p.then(() => {\n          audioUnlocked = true;\n          // Remove listeners once successfully unlocked\n          unlockEvents.forEach(event => {\n            window.removeEventListener(event, unlockHandler, { capture: true });\n          });\n        }).catch(_e => {\n          // If it fails (e.g. rapid clicks), we just keep the listeners attached\n          // and try again on next interaction","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__lib__audio__part-01.mp3"}
{"input":"        });\n      }\n    };\n\n    // Use capture: true to catch the event as early as possible\n    unlockEvents.forEach(event => {\n      window.addEventListener(event, unlockHandler, { capture: true, passive: false });\n    });\n  }\n\n  if (!flipAudio) {\n    try {\n      flipAudio = new Audio('/sounds/flip.mp3');\n      flipAudio.preload = 'auto';\n    } catch {\n      flipAudio = null;\n    }\n  }\n\n  if (!ambienceAudio) {\n    try {\n      ambienceAudio = new Audio('/sounds/ambience.mp3');\n      ambienceAudio.loop = true;\n      ambienceAudio.volume = AMBIENCE_BASE_VOLUME;\n    } catch {\n      ambienceAudio = null;\n    }\n  }\n\n  return {\n    flipAudio,\n    ambienceAudio\n  };\n}\n\nexport function playFlip() {\n  if (!flipAudio) return;\n  try {\n    flipAudio.currentTime = 0;\n    void flipAudio.play();\n  } catch {\n    // ignore autoplay / interruption errors\n  }\n}\n\nexport function toggleAmbience(on) {\n  if (!ambienceAudio) return;\n  try {\n    if (on) {\n      void ambienceAudio.play();\n    } else {\n      ambienceAudio.pause();\n    }\n  } catch {\n    // ignore autoplay / interruption errors\n  }\n}\n\nfunction clampVolume(value) {\n  if (!Number.isFinite(value)) return AMBIENCE_BASE_VOLUME;\n  return Math.min(1, Math.max(0, value));\n}\n\nfunction rampAmbienceVolume(from, to, durationMs) {\n  if (!ambienceAudio) return;\n  if (typeof window === 'undefined') return;\n  if (ambienceVolumeRaf) {\n    cancelAnimationFrame(ambienceVolumeRaf);\n    ambienceVolumeRaf = null;\n  }\n\n  const start = typeof performance !== 'undefined' ? performance.now() : Date.now();\n  const safeDuration = Math.max(80, durationMs || 0);\n  const startVolume = clampVolume(from);\n  const endVolume = clampVolume(to);\n\n  const step = (now) => {\n    const elapsed = now - start;\n    const t = Math.min(1, elapsed / safeDuration);\n    const eased = 1 - Math.pow(1 - t, 2);\n    ambienceAudio.volume = startVolume + (endVolume - startVolume) * eased;\n\n    if (t < 1) {\n      ambienceVolumeRaf = requestAnimationFrame(step);\n    } else {\n      ambienceVolumeRaf = null;\n    }\n  };\n\n  ambienceVolumeRaf = requestAnimationFrame(step);\n}\n\nexport function swellAmbience({ peak = 0.32, attackMs = 180, releaseMs = 700 } = {}) {\n  if (!ambienceAudio) return;\n  if (ambienceAudio.paused) return;\n\n  const baseVolume = clampVolume(ambienceAudio.volume || AMBIENCE_BASE_VOLUME);\n  const targetVolume = Math.min(1, Math.max(baseVolume, peak));\n\n  rampAmbienceVolume(baseVolume, targetVolume, attackMs);\n\n  if (ambienceSwellTimer) {\n    clearTimeout(ambienceSwellTimer);\n  }\n\n  ambienceSwellTimer = setTimeout(() => {\n    rampAmbienceVolume(targetVolume, baseVolume, releaseMs);\n    ambienceSwellTimer = null;\n  }, Math.max(120, attackMs));\n}\n\nfunction normalizeAudioContentType(contentType) {\n  if (!contentType) return 'audio/mpeg';\n  const normalized = contentType.toLowerCase();\n  if (normalized.includes('audio/mp3')) return 'audio/mpeg';\n  return contentType;\n}\n\nasync function buildStreamingAudioSource(response, signal) {\n  const contentType = normalizeAudioContentType(response.headers.get('content-type'));\n  const canStream = typeof MediaSource !== 'undefined' &&\n    typeof MediaSource.isTypeSupported === 'function' &&\n    response.body &&\n    MediaSource.isTypeSupported(contentType);\n\n  if (!canStream) {\n    const audioBlob = await response.blob();\n    const objectUrl = URL.createObjectURL(audioBlob);\n    trackObjectUrl(objectUrl);\n    return { url: objectUrl, source: 'blob' };\n  }\n\n  const mediaSource = new MediaSource();\n  const objectUrl = URL.createObjectURL(mediaSource);\n  trackObjectUrl(objectUrl);\n\n  const reader = response.body.getReader();\n  const queue = [];\n  let streamDone = false;\n  let sourceBuffer = null;\n\n  const pumpQueue = () => {\n    if (!sourceBuffer || sourceBuffer.updating || queue.length === 0) return;\n    const chunk = queue.shift();\n    if (chunk) {\n      sourceBuffer.appendBuffer(chunk);\n    }\n  };\n","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__lib__audio__part-02.mp3"}
{"input":"  const finishStream = () => {\n    if (mediaSource.readyState === 'open') {\n      try {\n        mediaSource.endOfStream();\n      } catch {\n        // Ignore end-of-stream errors.\n      }\n    }\n  };\n\n  const readLoop = async () => {\n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n        if (value) {\n          queue.push(value);\n          pumpQueue();\n        }\n      }\n      streamDone = true;\n      if (!sourceBuffer?.updating && queue.length === 0) {\n        finishStream();\n      }\n    } catch {\n      finishStream();\n    }\n  };\n\n  mediaSource.addEventListener('sourceopen', () => {\n    try {\n      sourceBuffer = mediaSource.addSourceBuffer(contentType);\n    } catch {\n      reader.cancel().catch(() => null);\n      finishStream();\n      return;\n    }\n\n    sourceBuffer.addEventListener('updateend', () => {\n      pumpQueue();\n      if (streamDone && queue.length === 0) {\n        finishStream();\n      }\n    });\n\n    void readLoop();\n  }, { once: true });\n\n  if (signal) {\n    signal.addEventListener('abort', () => {\n      reader.cancel().catch(() => null);\n      finishStream();\n    }, { once: true });\n  }\n\n  return { url: objectUrl, source: 'media-source' };\n}\n\n/**\n * Enhanced text-to-speech with intelligent caching and context-aware narration.\n *\n * Normalizes Markdown text before speaking to avoid \"asterisk asterisk\" narration\n * and create a natural, human storyteller voice.\n *\n * @param {Object} options\n * @param {string} options.text - Text to speak (can be Markdown)\n * @param {boolean} options.enabled - Whether TTS is enabled\n * @param {string} [options.context='default'] - Reading context (card-reveal, full-reading, synthesis, etc.)\n * @param {string} [options.voice='verse'] - Voice selection (alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, shimmer, verse)\n * @param {number} [options.speed] - Playback speed (0.25-4.0, default 1.1 for engaging pace)\n * @param {string} [options.format] - Audio format to request (mp3, wav, etc.)\n * @param {boolean} [options.stream=false] - Use streaming mode for progressive audio playback\n * @param {string} [options.emotion=null] - Emotion from GraphRAG analysis for voice styling\n */\nexport async function speakText({ text, enabled, context = 'default', voice = 'verse', speed, format, stream = false, emotion = null }) {\n  ensureGlobalCleanupListeners();\n\n  if (!enabled) {\n    emitTTSState({ status: 'idle', reason: 'disabled', message: null });\n    return;\n  }\n  if (!text || !text.trim()) {\n    emitTTSState({ status: 'idle', reason: 'no-text', message: null });\n    return;\n  }\n  if (typeof window === 'undefined' || typeof Audio === 'undefined') {\n    emitTTSState({ status: 'error', error: 'Audio playback not supported in this environment.' });\n    return;\n  }\n\n  const narrationContext = context || 'default';\n  const normalizedText = normalizeReadingText(text);\n  const ttsText = prepareForTTS(normalizedText);\n\n  const requestId = ++currentNarrationRequestId;\n  resetTTSStream({ preserveAudio: true });\n  if (ttsAbortController) {\n    ttsAbortController.abort();\n  }\n  const controller = typeof AbortController !== 'undefined' ? new AbortController() : null;\n  ttsAbortController = controller;\n  activeNarrationId = requestId;\n  const isStaleRequest = () => (activeNarrationId !== requestId) || Boolean(controller?.signal?.aborted);\n\n  try {\n    // Stop any currently playing TTS\n    if (ttsAudio) {\n      releaseAudioObjectUrl(ttsAudio);\n      ttsAudio.pause();\n      emitTTSState({ status: 'stopped', reason: 'replaced' });\n      ttsAudio = null;\n    }\n\n    // Check cache first (using normalized text for consistent keys)\n    // Include speed and format in cache key to cache variations separately\n    const cacheKey = generateCacheKey(ttsText, context, voice, speed, format, emotion);\n    const cachedAudio = getCachedAudio(cacheKey);\n","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__lib__audio__part-03.mp3"}
{"input":"    let audioDataUri;\n    let provider = cachedAudio?.provider || null;\n    let source = cachedAudio ? 'cache' : 'network';\n    let objectUrlForCleanup = null;\n\n    emitTTSState({\n      status: 'loading',\n      provider,\n      source,\n      cached: !!cachedAudio,\n      error: null,\n      message: cachedAudio ? 'Loading narration from cache.' : 'Preparing narration...',\n      context: narrationContext\n    });\n\n    if (cachedAudio && !stream) {\n      // Use cached audio (streaming responses are not cached)\n      audioDataUri = cachedAudio.audio;\n    } else {\n      // Fetch from API with normalized TTS text\n      const url = stream ? '/api/tts?stream=true' : '/api/tts';\n      const requestBody = { text: ttsText, context, voice };\n\n      // Add speed parameter if specified\n      if (speed !== undefined) {\n        requestBody.speed = speed;\n      }\n      if (typeof format === 'string' && format.trim()) {\n        requestBody.format = format.trim();\n      }\n      // Add emotion for voice styling if provided\n      if (emotion) {\n        requestBody.emotion = emotion;\n      }\n\n      const response = await fetch(url, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify(requestBody),\n        signal: controller?.signal\n      });\n\n      if (isStaleRequest()) {\n        return;\n      }\n\n      if (!response.ok) {\n        console.error('TTS error:', response.status);\n\n        // Parse error response for details\n        let errorData = {};\n        try {\n          errorData = await response.json();\n        } catch {\n          // Not JSON, use status text\n        }\n\n        const errorCode = errorData.errorCode || (response.status === 429 ? 'RATE_LIMIT' : 'SERVICE_ERROR');\n        const userMessage = getErrorMessage(response.status, errorData);\n\n        if (isStaleRequest()) {\n          return;\n        }\n\n        emitTTSState({\n          status: 'error',\n          provider,\n          source,\n          error: errorData.error || `TTS service returned ${response.status}.`,\n          errorCode,\n          errorDetails: errorData,\n          context: narrationContext,\n          message: userMessage\n        });\n        return;\n      }\n\n      if (stream) {\n        // Streaming mode: response body is a ReadableStream of audio chunks\n        const streamResult = await buildStreamingAudioSource(response, controller?.signal);\n        audioDataUri = streamResult.url;\n        objectUrlForCleanup = streamResult.url;\n        const headerProvider = response.headers.get('x-tts-provider');\n        provider = headerProvider || provider || 'azure-gpt-4o-mini-tts';\n        source = 'stream';\n        if (isStaleRequest()) {\n          revokeTrackedObjectUrl(streamResult.url);\n          return;\n        }\n      } else {\n        // Non-streaming mode: response is JSON with base64 data URI\n        const data = await response.json();\n        if (isStaleRequest()) {\n          return;\n        }\n        if (!data?.audio) {\n          console.error('No audio field in TTS response');\n          emitTTSState({\n            status: 'error',\n            provider: data?.provider || null,\n            source,\n            error: 'No audio field in response.',\n            context: narrationContext,\n            message: 'Unable to prepare audio for this reading.'\n          });\n          return;\n        }\n\n        audioDataUri = data.audio;\n        provider = data?.provider || null;\n\n        // Cache the audio for future use, but never cache fallback provider\n        if (!isStaleRequest() && provider && provider !== 'fallback') {\n          cacheAudio(cacheKey, audioDataUri, provider);\n        }\n      }\n\n      if (isStaleRequest()) {\n        if (objectUrlForCleanup) {\n          revokeTrackedObjectUrl(objectUrlForCleanup);\n        }\n        return;\n      }\n\n      emitTTSState({\n        status: 'loading',\n        provider,","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__lib__audio__part-04.mp3"}
{"input":"        source: stream ? 'stream' : source,\n        cached: false,\n        error: null,\n        context: narrationContext,\n        message: getPreparingMessage(provider, narrationContext)\n      });\n    }\n\n    if (isStaleRequest() || requestId <= cancelledUpToRequestId) {\n      emitTTSState({\n        status: 'stopped',\n        reason: 'user',\n        context: narrationContext,\n        message: 'Narration stopped.'\n      });\n      activeNarrationId = null;\n      return;\n    }\n\n    // Ensure audio is unlocked before attempting playback\n    if (!audioUnlocked) {\n      const unlocked = await unlockAudio();\n      if (!unlocked) {\n        if (isStaleRequest()) {\n          activeNarrationId = null;\n          return;\n        }\n        emitTTSState({\n          status: 'unlock-failed',\n          provider,\n          source,\n          context: narrationContext,\n          error: 'Audio not unlocked',\n          message: 'Tap anywhere on the page to enable audio, then try again.',\n          reason: 'interaction-required'\n        });\n        activeNarrationId = null;\n        return;\n      }\n    }\n\n    if (isStaleRequest()) {\n      activeNarrationId = null;\n      return;\n    }\n\n    // Play the audio\n    const audio = new Audio(audioDataUri);\n    // Important: set volume based on ambience setting or default\n    audio.volume = 1.0;\n\n    if (objectUrlForCleanup) {\n      audioObjectUrlMap.set(audio, objectUrlForCleanup);\n    }\n    ttsAudio = audio;\n    wireTTSEvents(audio, provider, source, requestId, narrationContext);\n\n    try {\n      if (isStaleRequest()) {\n        releaseAudioObjectUrl(audio);\n        return;\n      }\n      await audio.play();\n      if (isStaleRequest()) {\n        releaseAudioObjectUrl(audio);\n        return;\n      }\n      emitTTSState({\n        status: 'playing',\n        provider,\n        source,\n        context: narrationContext,\n        message: getPlayMessage(provider, narrationContext)\n      });\n    } catch (err) {\n      console.error('Error playing TTS audio:', err);\n      const isAutoplayError = err.name === 'NotAllowedError' ||\n        err.message?.toLowerCase().includes('autoplay') ||\n        err.message?.toLowerCase().includes('user interaction');\n\n      emitTTSState({\n        status: 'error',\n        provider,\n        source,\n        context: narrationContext,\n        error: err?.message || String(err),\n        message: isAutoplayError\n          ? 'Tap anywhere on the page to enable audio, then try again.'\n          : 'Unable to play audio. Please try again.'\n      });\n      activeNarrationId = null;\n    }\n  } catch (err) {\n    if (controller?.signal?.aborted || err?.name === 'AbortError' || isStaleRequest()) {\n      return;\n    }\n    console.error('Error playing TTS audio:', err);\n    const fallbackPlayed = await tryPlayLocalFallback({\n      requestId,\n      context: narrationContext,\n      fallbackText: normalizedText || text\n    });\n\n    if (fallbackPlayed) {\n      return;\n    }\n\n    emitTTSState({\n      status: 'error',\n      context: narrationContext,\n      error: err?.message || String(err),\n      message: 'Unable to play audio right now.'\n    });\n    activeNarrationId = null;\n  }\n}\n\nexport function enqueueTTSChunk({ text, context = 'full-reading', voice = 'nova', speed, format, emotion = null }) {\n  if (!text || typeof text !== 'string') return false;\n  ensureGlobalCleanupListeners();\n\n  const normalizedText = normalizeReadingText(text);\n  const ttsText = prepareForTTS(normalizedText);\n  if (!ttsText.trim()) return false;\n\n  if (!ttsStreamActive) {\n    ttsStreamActive = true;\n    ttsStreamFinalized = false;\n    ttsStreamRequestId = ++currentNarrationRequestId;\n    activeNarrationId = ttsStreamRequestId;\n  }\n\n  ttsStreamQueue.push({ text: ttsText, context, voice, speed, format, emotion });\n\n  if (!ttsStreamProcessing) {\n    void processTtsStreamQueue();\n  }\n\n  return true;\n}\n\nexport function finalizeTTSStream() {","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__lib__audio__part-05.mp3"}
{"input":"  ttsStreamFinalized = true;\n  if (!ttsStreamProcessing && ttsStreamActive && ttsStreamQueue.length === 0) {\n    finishTtsStreamQueue();\n  }\n}\n\nexport function resetTTSStream({ preserveAudio = false } = {}) {\n  clearTtsStreamState({ preserveAudio });\n}\n\nexport function isTTSStreamActive() {\n  return ttsStreamActive;\n}\n\nfunction clearTtsStreamState({ preserveAudio = false } = {}) {\n  if (ttsStreamRequestId) {\n    cancelledUpToRequestId = Math.max(cancelledUpToRequestId, ttsStreamRequestId);\n  }\n  ttsStreamActive = false;\n  ttsStreamFinalized = false;\n  ttsStreamProcessing = false;\n  ttsStreamQueue.length = 0;\n  if (ttsStreamAbortController) {\n    ttsStreamAbortController.abort();\n    ttsStreamAbortController = null;\n  }\n  if (!preserveAudio && ttsAudio) {\n    releaseAudioObjectUrl(ttsAudio);\n    ttsAudio.pause();\n    ttsAudio.currentTime = 0;\n    ttsAudio = null;\n  }\n}\n\nasync function processTtsStreamQueue() {\n  if (ttsStreamProcessing || !ttsStreamActive) return;\n  ttsStreamProcessing = true;\n  const requestId = ttsStreamRequestId;\n\n  while (ttsStreamQueue.length > 0) {\n    if (!ttsStreamActive || requestId !== ttsStreamRequestId || requestId <= cancelledUpToRequestId) {\n      break;\n    }\n    const segment = ttsStreamQueue.shift();\n    try {\n      await playTtsStreamSegment(segment, requestId);\n    } catch (err) {\n      if (requestId !== ttsStreamRequestId || requestId <= cancelledUpToRequestId) {\n        break;\n      }\n      emitTTSState({\n        status: 'error',\n        error: err?.message || String(err),\n        message: 'Unable to play audio right now.'\n      });\n      clearTtsStreamState({ preserveAudio: false });\n      break;\n    }\n  }\n\n  ttsStreamProcessing = false;\n\n  if (ttsStreamActive && ttsStreamFinalized && ttsStreamQueue.length === 0 && requestId === ttsStreamRequestId) {\n    finishTtsStreamQueue();\n  }\n}\n\nasync function playTtsStreamSegment(segment, requestId) {\n  if (!segment || requestId <= cancelledUpToRequestId) return;\n\n  const { text, context = 'full-reading', voice = 'nova', speed, format, emotion } = segment;\n\n  if (!audioUnlocked) {\n    const unlocked = await unlockAudio();\n    if (!unlocked) {\n      throw new Error('Audio not unlocked');\n    }\n  }\n\n  if (requestId <= cancelledUpToRequestId) return;\n\n  emitTTSState({\n    status: 'loading',\n    provider: null,\n    source: 'stream',\n    context,\n    message: getPreparingMessage(null, context)\n  });\n\n  if (ttsStreamAbortController) {\n    ttsStreamAbortController.abort();\n  }\n  const controller = typeof AbortController !== 'undefined' ? new AbortController() : null;\n  ttsStreamAbortController = controller;\n\n  const requestBody = { text, context, voice };\n  if (speed !== undefined) {\n    requestBody.speed = speed;\n  }\n  if (typeof format === 'string' && format.trim()) {\n    requestBody.format = format.trim();\n  }\n  if (emotion) {\n    requestBody.emotion = emotion;\n  }\n\n  const response = await fetch('/api/tts?stream=true', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify(requestBody),\n    signal: controller?.signal\n  });\n\n  if (!response.ok) {\n    let errorData = {};\n    try {\n      errorData = await response.json();\n    } catch {\n      errorData = {};\n    }\n    const errorCode = errorData.errorCode || (response.status === 429 ? 'RATE_LIMIT' : 'SERVICE_ERROR');\n    const userMessage = getErrorMessage(response.status, errorData);\n    emitTTSState({\n      status: 'error',\n      error: errorData.error || `TTS service returned ${response.status}.`,\n      errorCode,\n      errorDetails: errorData,\n      context,\n      message: userMessage\n    });\n    throw new Error(errorData.error || userMessage);\n  }\n\n  const streamResult = await buildStreamingAudioSource(response, controller?.signal);\n  const headerProvider = response.headers.get('x-tts-provider');\n  const provider = headerProvider || 'azure-gpt-4o-mini-tts';","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__lib__audio__part-06.mp3"}
{"input":"  const source = 'stream';\n\n  emitTTSState({\n    status: 'loading',\n    provider,\n    source,\n    cached: false,\n    context,\n    message: getPreparingMessage(provider, context)\n  });\n\n  await playQueuedAudio(streamResult.url, {\n    provider,\n    source,\n    context,\n    requestId,\n    signal: controller?.signal\n  });\n}\n\nfunction playQueuedAudio(audioUrl, { provider, source, context, requestId, signal }) {\n  return new Promise((resolve, reject) => {\n    const audio = new Audio(audioUrl);\n    ttsAudio = audio;\n    audioObjectUrlMap.set(audio, audioUrl);\n\n    const cleanup = ({ stop = false } = {}) => {\n      audio.removeEventListener('loadedmetadata', handleLoadedMetadata);\n      audio.removeEventListener('timeupdate', handleTimeUpdate);\n      audio.removeEventListener('ended', handleEnded);\n      audio.removeEventListener('pause', handlePause);\n      audio.removeEventListener('error', handleError);\n      if (signal) {\n        signal.removeEventListener('abort', handleAbort);\n      }\n      if (stop) {\n        try {\n          audio.pause();\n          audio.currentTime = 0;\n        } catch {\n          // ignore stop errors\n        }\n      }\n      releaseAudioObjectUrl(audio);\n      if (ttsAudio === audio) {\n        ttsAudio = null;\n      }\n      if (activeNarrationId === requestId && ttsStreamQueue.length === 0 && ttsStreamFinalized) {\n        activeNarrationId = null;\n      }\n    };\n\n    const handleAbort = () => {\n      cleanup({ stop: true });\n      resolve();\n    };\n\n    const handleLoadedMetadata = () => {\n      const duration = audio.duration || 0;\n      if (duration > 0 && isFinite(duration)) {\n        emitTTSState({ duration });\n      }\n    };\n\n    const handleTimeUpdate = () => {\n      const currentTime = audio.currentTime || 0;\n      const duration = audio.duration || 0;\n      const progress = duration > 0 ? Math.min(currentTime / duration, 1) : 0;\n      emitTTSState({ currentTime, duration, progress });\n    };\n\n    const handleEnded = () => {\n      cleanup();\n      resolve();\n    };\n\n    const handlePause = () => {\n      if (!audio.ended) {\n        emitTTSState({\n          status: 'paused',\n          provider,\n          source,\n          context,\n          message: getPauseMessage(provider, context)\n        });\n      }\n    };\n\n    const handleError = () => {\n      emitTTSState({\n        status: 'error',\n        provider,\n        source,\n        context,\n        error: 'Audio playback error.',\n        errorCode: 'PLAYBACK_ERROR',\n        message: 'Something went wrong while playing audio.'\n      });\n      cleanup();\n      reject(new Error('Audio playback error.'));\n    };\n\n    audio.addEventListener('loadedmetadata', handleLoadedMetadata);\n    audio.addEventListener('timeupdate', handleTimeUpdate);\n    audio.addEventListener('ended', handleEnded);\n    audio.addEventListener('pause', handlePause);\n    audio.addEventListener('error', handleError);\n\n    if (signal) {\n      if (signal.aborted) {\n        handleAbort();\n        return;\n      }\n      signal.addEventListener('abort', handleAbort, { once: true });\n    }\n\n    audio.play()\n      .then(() => {\n        emitTTSState({\n          status: 'playing',\n          provider,\n          source,\n          context,\n          message: getPlayMessage(provider, context)\n        });\n      })\n      .catch((err) => {\n        handleError(err);\n      });\n  });\n}\n\nfunction finishTtsStreamQueue() {\n  if (!ttsStreamActive) return;\n  ttsStreamActive = false;\n  ttsStreamFinalized = false;\n  const provider = currentTTSState.provider || 'azure-gpt-4o-mini-tts';\n  const context = currentTTSState.context || 'full-reading';\n  emitTTSState({\n    status: 'completed',\n    provider,\n    source: currentTTSState.source || 'stream',\n    context,\n    message: getEndedMessage(provider, context),\n    currentTime: 0,\n    duration: 0,\n    progress: 0\n  });\n  if (activeNarrationId === ttsStreamRequestId) {","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__lib__audio__part-07.mp3"}
{"input":"    activeNarrationId = null;\n  }\n  ttsStreamRequestId = 0;\n}\n\nasync function tryPlayLocalFallback({ requestId, context, fallbackText }) {\n  try {\n    const safeText = fallbackText && fallbackText.length\n      ? fallbackText\n      : 'The cards rest quietly; here is a gentle chime instead.';\n    const audioDataUri = generateFallbackWaveform(safeText);\n\n    emitTTSState({\n      status: 'loading',\n      provider: 'fallback',\n      source: 'local',\n      cached: false,\n      error: null,\n      context,\n      message: getPreparingMessage('fallback', context)\n    });\n\n    if (requestId <= cancelledUpToRequestId) {\n      emitTTSState({\n        status: 'stopped',\n        reason: 'user',\n        context,\n        message: 'Narration stopped.'\n      });\n      activeNarrationId = null;\n      return true;\n    }\n\n    if (!audioUnlocked) {\n      const unlocked = await unlockAudio();\n      if (!unlocked) {\n        emitTTSState({\n          status: 'unlock-failed',\n          provider: 'fallback',\n          source: 'local',\n          context,\n          error: 'Audio not unlocked',\n          message: 'Tap anywhere on the page to enable audio, then try again.',\n          reason: 'interaction-required'\n        });\n        activeNarrationId = null;\n        return true;\n      }\n    }\n\n    const fallbackAudio = new Audio(audioDataUri);\n    ttsAudio = fallbackAudio;\n    wireTTSEvents(fallbackAudio, 'fallback', 'local', requestId, context);\n    await fallbackAudio.play();\n\n    emitTTSState({\n      status: 'playing',\n      provider: 'fallback',\n      source: 'local',\n      context,\n      message: getPlayMessage('fallback', context)\n    });\n\n    return true;\n  } catch (fallbackErr) {\n    console.error('Local fallback audio failed:', fallbackErr);\n    return false;\n  }\n}\n\n/**\n * Generate a cache key from text, context, voice, speed, format, and emotion.\n * Uses djb2 hash to keep localStorage keys reasonable.\n */\nfunction generateCacheKey(text, context, voice, speed, format, emotion) {\n  const speedKey = speed !== undefined ? speed : 'default';\n  const formatKey = format && String(format).trim().length ? String(format).trim().toLowerCase() : 'default';\n  const emotionKey = emotion && String(emotion).trim().length ? String(emotion).trim().toLowerCase() : 'default';\n  const content = `${text}|${context}|${voice}|${speedKey}|${formatKey}|${emotionKey}`;\n  return `${TTS_CACHE_PREFIX}${djb2Hash(content).toString(36)}`;\n}\n\n/**\n * Get cached audio from localStorage.\n * Returns null if not found or cache is stale.\n */\nfunction getCachedAudio(key) {\n  try {\n    if (!safeStorage.isAvailable) return null;\n\n    const cached = safeStorage.getItem(key);\n    if (!cached) return null;\n\n    const data = JSON.parse(cached);\n    const now = Date.now();\n    const cacheAge = now - data.timestamp;\n\n    // Invalidate stale cache\n    if (cacheAge > CACHE_MAX_AGE_MS) {\n      safeStorage.removeItem(key);\n      return null;\n    }\n\n    return data;\n  } catch (err) {\n    console.warn('Error reading TTS cache:', err);\n    return null;\n  }\n}\n\n/**\n * Cache audio data URI in localStorage.\n * Implements size limit and eviction strategy.\n */\nfunction cacheAudio(key, audioDataUri, provider = null) {\n  try {\n    if (!safeStorage.isAvailable) return;\n\n    const data = {\n      audio: audioDataUri,\n      timestamp: Date.now(),\n      provider\n    };\n    const payload = JSON.stringify(data);\n    let entries = collectCacheEntries();\n    maybeSweepCache(entries.length);\n\n    if (entries.length >= TTS_CACHE_MAX_ENTRIES) {\n      evictOldestEntries(entries, TTS_CACHE_EVICT_BATCH);\n      entries = entries.slice(TTS_CACHE_EVICT_BATCH);\n    }\n\n    try {\n      safeStorage.setItem(key, payload);\n    } catch (err) {\n      if (isQuotaExceededError(err)) {\n        entries = collectCacheEntries();\n        const removalTarget = Math.max(TTS_CACHE_EVICT_BATCH, entries.length - TTS_CACHE_PURGE_TARGET);","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__lib__audio__part-08.mp3"}
{"input":"        if (removalTarget > 0) {\n          evictOldestEntries(entries, removalTarget);\n        } else {\n          clearTTSCache({ keepLatest: TTS_CACHE_PURGE_TARGET });\n        }\n\n        try {\n          safeStorage.setItem(key, payload);\n          return;\n        } catch (retryErr) {\n          console.warn('Unable to cache TTS audio after eviction:', retryErr);\n        }\n      } else {\n        console.warn('Unable to cache TTS audio:', err);\n      }\n    }\n  } catch (err) {\n    // localStorage full or unavailable - just continue without caching\n    console.warn('Unable to cache TTS audio:', err);\n  }\n}\n\nexport function clearTTSCache({ keepLatest = 0 } = {}) {\n  try {\n    if (!safeStorage.isAvailable) return;\n    const entries = collectCacheEntries();\n    if (!entries.length) return;\n\n    if (keepLatest <= 0) {\n      for (const entry of entries) {\n        safeStorage.removeItem(entry.key);\n      }\n      return;\n    }\n\n    if (keepLatest >= entries.length) {\n      return;\n    }\n\n    const removeCount = entries.length - keepLatest;\n    for (let i = 0; i < removeCount; i += 1) {\n      safeStorage.removeItem(entries[i].key);\n    }\n  } catch (err) {\n    console.warn('Failed to clear TTS cache:', err);\n  }\n}\n\nfunction collectCacheEntries() {\n  if (!safeStorage.isAvailable) return [];\n\n  // Need direct localStorage access for Object.keys()\n  return Object.keys(localStorage)\n    .filter(key => key.startsWith(TTS_CACHE_PREFIX))\n    .map(key => {\n      try {\n        const value = JSON.parse(safeStorage.getItem(key));\n        return { key, timestamp: value?.timestamp || 0 };\n      } catch {\n        return { key, timestamp: 0 };\n      }\n    })\n    .sort((a, b) => a.timestamp - b.timestamp);\n}\n\nfunction evictOldestEntries(entries, count) {\n  if (!safeStorage.isAvailable) return;\n  const toRemove = Math.min(count, entries.length);\n  for (let i = 0; i < toRemove; i += 1) {\n    safeStorage.removeItem(entries[i].key);\n  }\n}\n\nfunction maybeSweepCache(entryCount) {\n  if (entryCount < TTS_CACHE_PURGE_THRESHOLD) return;\n  const now = Date.now();\n  if (now - lastCacheSweep < TTS_CACHE_SWEEP_INTERVAL_MS) {\n    return;\n  }\n  lastCacheSweep = now;\n  clearTTSCache({ keepLatest: TTS_CACHE_PURGE_TARGET });\n}\n\nfunction isQuotaExceededError(err) {\n  if (!err) return false;\n  const name = err.name || '';\n  return name === 'QuotaExceededError' || name === 'NS_ERROR_DOM_QUOTA_REACHED';\n}\n\n/**\n * Stop any currently playing TTS audio.\n * This is called when the user toggles voice off in settings.\n */\nexport function stopTTS() {\n  try {\n    resetTTSStream({ preserveAudio: true });\n    const targetId = activeNarrationId ?? currentNarrationRequestId;\n    if (targetId) {\n      cancelledUpToRequestId = Math.max(cancelledUpToRequestId, targetId);\n    }\n    activeNarrationId = null;\n    if (ttsAbortController) {\n      ttsAbortController.abort();\n      ttsAbortController = null;\n    }\n\n    if (ttsAudio) {\n      releaseAudioObjectUrl(ttsAudio);\n      ttsAudio.pause();\n      ttsAudio.currentTime = 0;\n      ttsAudio = null;\n      emitTTSState({\n        status: 'stopped',\n        reason: 'user',\n        message: 'Narration stopped.'\n      });\n    } else {\n      emitTTSState({\n        status: 'stopped',\n        reason: 'user',\n        message: 'Narration stopped.'\n      });\n    }\n  } catch {\n    // ignore errors\n  }\n}\n\nexport function cleanupAudio() {\n  try {\n    resetTTSStream({ preserveAudio: true });\n    if (ambienceSwellTimer) {\n      clearTimeout(ambienceSwellTimer);\n      ambienceSwellTimer = null;\n    }\n    if (ambienceVolumeRaf) {\n      cancelAnimationFrame(ambienceVolumeRaf);\n      ambienceVolumeRaf = null;\n    }\n    if (flipAudio) {\n      flipAudio.pause();\n      flipAudio = null;\n    }\n    if (ambienceAudio) {\n      ambienceAudio.pause();\n      ambienceAudio = null;\n    }\n    if (ttsAudio) {\n      releaseAudioObjectUrl(ttsAudio);\n      ttsAudio.pause();","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__lib__audio__part-09.mp3"}
{"input":"      ttsAudio = null;\n    }\n    if (ttsAbortController) {\n      ttsAbortController.abort();\n      ttsAbortController = null;\n    }\n  } catch {\n    // ignore cleanup errors\n  }\n\n  emitTTSState({\n    status: 'idle',\n    provider: null,\n    source: null,\n    cached: false,\n    error: null,\n    message: null,\n    context: null\n  });\n}\n\nexport function pauseTTS() {\n  if (!ttsAudio) return;\n  try {\n    if (!ttsAudio.paused) {\n      ttsAudio.pause();\n      emitTTSState({\n        status: 'paused',\n        reason: 'user',\n        message: currentTTSState.provider === 'fallback'\n          ? 'Fallback chime paused.'\n          : 'Narration paused.'\n      });\n    }\n  } catch {\n    // ignore pause errors\n  }\n}\n\nexport async function resumeTTS() {\n  if (!ttsAudio) return;\n  try {\n    await ttsAudio.play();\n    emitTTSState({\n      status: 'playing',\n      reason: 'resume',\n      message: currentTTSState.provider === 'fallback'\n        ? 'Resuming fallback chime.'\n        : 'Resuming narration.'\n    });\n  } catch (err) {\n    emitTTSState({\n      status: 'error',\n      error: err?.message || String(err),\n      message: 'Unable to resume audio. Tap the page, then try again.'\n    });\n  }\n}\n\nexport function subscribeToTTS(listener) {\n  if (typeof listener !== 'function') return () => { };\n  ttsListeners.add(listener);\n  listener(currentTTSState);\n  return () => {\n    ttsListeners.delete(listener);\n  };\n}\n\nexport function getCurrentTTSState() {\n  return currentTTSState;\n}\n\nfunction emitTTSState(update) {\n  const nextStatus = update.status ?? currentTTSState.status;\n  const isResetState = nextStatus === 'loading' || nextStatus === 'idle';\n  const isErrorState = nextStatus === 'error';\n\n  currentTTSState = {\n    status: nextStatus,\n    provider: update.provider ?? (isResetState ? null : currentTTSState.provider) ?? null,\n    source: update.source ?? (isResetState ? null : currentTTSState.source) ?? null,\n    cached: update.cached ?? (isResetState ? false : currentTTSState.cached) ?? false,\n    error: isErrorState\n      ? (update.error ?? currentTTSState.error ?? null)\n      : null,\n    errorCode: isErrorState\n      ? (update.errorCode ?? currentTTSState.errorCode ?? null)\n      : null,\n    errorDetails: isErrorState\n      ? (update.errorDetails ?? currentTTSState.errorDetails ?? null)\n      : null,\n    message: update.message ??\n      (isErrorState\n        ? (currentTTSState.message ?? null)\n        : (isResetState ? null : currentTTSState.message ?? null)),\n    reason: update.reason ?? (isResetState ? null : currentTTSState.reason) ?? null,\n    context: update.context ?? (isResetState ? null : currentTTSState.context) ?? null,\n    // Progress tracking - reset on loading/idle, otherwise preserve or update\n    currentTime: update.currentTime ?? (isResetState ? 0 : currentTTSState.currentTime) ?? 0,\n    duration: update.duration ?? (isResetState ? 0 : currentTTSState.duration) ?? 0,\n    progress: update.progress ?? (isResetState ? 0 : currentTTSState.progress) ?? 0\n  };\n\n  for (const listener of ttsListeners) {\n    try {\n      listener(currentTTSState);\n    } catch (err) {\n      console.warn('TTS listener error:', err);\n    }\n  }\n}\n\nfunction wireTTSEvents(audio, provider, source, requestId, context) {\n  // Progress tracking: capture duration when metadata loads\n  audio.addEventListener('loadedmetadata', () => {\n    const duration = audio.duration || 0;\n    if (duration > 0 && isFinite(duration)) {\n      emitTTSState({ duration });\n    }\n  });\n\n  // Progress tracking: update currentTime and progress on timeupdate\n  audio.addEventListener('timeupdate', () => {\n    const currentTime = audio.currentTime || 0;\n    const duration = audio.duration || 0;\n    const progress = duration > 0 ? Math.min(currentTime / duration, 1) : 0;\n    emitTTSState({ currentTime, duration, progress });\n  });\n\n  audio.addEventListener('ended', () => {\n    emitTTSState({","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__lib__audio__part-10.mp3"}
{"input":"      status: 'completed',\n      provider,\n      source,\n      context,\n      message: getEndedMessage(provider, context),\n      // Reset progress on completion\n      currentTime: 0,\n      duration: 0,\n      progress: 0\n    });\n    releaseAudioObjectUrl(audio);\n    if (ttsAudio === audio) {\n      ttsAudio = null;\n    }\n    if (activeNarrationId === requestId) {\n      activeNarrationId = null;\n    }\n  });\n\n  audio.addEventListener('pause', () => {\n    if (!audio.ended) {\n      emitTTSState({\n        status: 'paused',\n        provider,\n        source,\n        context,\n        message: getPauseMessage(provider, context)\n      });\n    }\n  });\n\n  audio.addEventListener('error', () => {\n    emitTTSState({\n      status: 'error',\n      provider,\n      source,\n      context,\n      error: 'Audio playback error.',\n      errorCode: 'PLAYBACK_ERROR',\n      message: 'Something went wrong while playing audio.'\n    });\n    releaseAudioObjectUrl(audio);\n    if (activeNarrationId === requestId) {\n      activeNarrationId = null;\n    }\n  });\n}\n\nfunction getPlayMessage(provider, context) {\n  if (provider === 'fallback') {\n    return 'Voice service unavailable; playing a gentle chime instead of narration.';\n  }\n  if (context === 'card-reveal') {\n    return 'Speaking this card reveal.';\n  }\n  if (context === 'full-reading') {\n    return 'Playing your personal reading narration.';\n  }\n  return 'Playing narration.';\n}\n\nfunction getPreparingMessage(provider, context) {\n  if (provider === 'fallback') {\n    return 'Preparing fallback chime.';\n  }\n  if (context === 'card-reveal') {\n    return 'Preparing card reveal narration.';\n  }\n  if (context === 'full-reading') {\n    return 'Preparing personal reading narration.';\n  }\n  return 'Preparing narration...';\n}\n\nfunction getEndedMessage(provider, context) {\n  if (provider === 'fallback') {\n    return 'Fallback chime finished.';\n  }\n  if (context === 'card-reveal') {\n    return 'Card narration finished.';\n  }\n  if (context === 'full-reading') {\n    return 'Personal reading narration finished.';\n  }\n  return 'Narration finished.';\n}\n\nfunction getPauseMessage(provider, context) {\n  if (provider === 'fallback') {\n    return 'Fallback chime paused.';\n  }\n  if (context === 'card-reveal') {\n    return 'Card narration paused.';\n  }\n  if (context === 'full-reading') {\n    return 'Personal reading narration paused.';\n  }\n  return 'Narration paused.';\n}\n\n/**\n * Get user-friendly error message based on status and error data.\n */\nfunction getErrorMessage(status, errorData) {\n  if (status === 429) {\n    if (errorData.tierLimited) {\n      const used = errorData.used ?? '?';\n      const limit = errorData.limit ?? '?';\n      return `Monthly limit reached (${used}/${limit}). Upgrade for more.`;\n    }\n    return 'Too many requests. Please wait a moment.';\n  }\n  if (status === 503 || status === 502) {\n    return 'Voice service temporarily unavailable. Try again shortly.';\n  }\n  if (status >= 500) {\n    return 'Voice service error. Using fallback if available.';\n  }\n  return 'Unable to generate audio right now.';\n}\n\nfunction ensureGlobalCleanupListeners() {\n  if (cleanupListenersRegistered || typeof window === 'undefined') {\n    return;\n  }\n  cleanupListenersRegistered = true;\n\n  const handleLifecycleEnd = () => {\n    revokeAllObjectUrls();\n    clearTTSCache();\n  };\n\n  window.addEventListener('beforeunload', handleLifecycleEnd);\n  window.addEventListener('pagehide', handleLifecycleEnd);\n}\n\nfunction trackObjectUrl(url) {\n  if (!url || typeof URL === 'undefined') {\n    return;\n  }\n  trackedObjectUrls.add(url);\n}\n\nfunction releaseAudioObjectUrl(audio) {\n  if (!audio) return;\n  const objectUrl = audioObjectUrlMap.get(audio);\n  if (objectUrl) {\n    revokeTrackedObjectUrl(objectUrl);\n    audioObjectUrlMap.delete(audio);\n  }\n}\n\nfunction revokeTrackedObjectUrl(url) {\n  if (!url || typeof URL === 'undefined') {\n    return;\n  }\n  try {","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__lib__audio__part-11.mp3"}
{"input":"    URL.revokeObjectURL(url);\n  } catch {\n    // Ignore browsers that throw on redundant revoke\n  }\n  trackedObjectUrls.delete(url);\n}\n\nfunction revokeAllObjectUrls() {\n  if (!trackedObjectUrls.size || typeof URL === 'undefined') {\n    return;\n  }\n  for (const url of trackedObjectUrls) {\n    try {\n      URL.revokeObjectURL(url);\n    } catch {\n      // Ignore browsers that throw on redundant revoke\n    }\n  }\n  trackedObjectUrls.clear();\n}\n","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__lib__audio__part-12.mp3"}
{"input":"import { jsonResponse } from '../lib/utils.js';\nimport { resolveEnv } from '../lib/environment.js';\nimport { getUserFromRequest } from '../lib/auth.js';\nimport { getSubscriptionContext } from '../lib/entitlements.js';\nimport { enforceApiCallLimit } from '../lib/apiUsage.js';\nimport { enforceTtsRateLimit, getTtsLimits } from '../lib/ttsLimits.js';\n\n/**\n * Speech Token Endpoint for Azure Cognitive Services Speech SDK\n *\n * Issues short-lived authorization tokens for client-side Speech SDK usage.\n * This keeps the subscription key secure on the server while allowing\n * the browser to synthesize speech directly with Azure.\n */\nexport async function onRequestGet(context) {\n  const { request, env } = context;\n  const requestId = crypto.randomUUID();\n\n  const user = await getUserFromRequest(request, env);\n  if (!user) {\n    return jsonResponse({ error: 'Not authenticated' }, { status: 401 });\n  }\n\n  // API key requests are subject to API call limits.\n  if (user?.auth_provider === 'api_key') {\n    const apiLimit = await enforceApiCallLimit(env, user);\n    if (!apiLimit.allowed) {\n      return jsonResponse(apiLimit.payload, { status: apiLimit.status });\n    }\n  }\n\n  const subscription = getSubscriptionContext(user);\n  const ttsLimits = getTtsLimits(subscription.effectiveTier);\n  const rateLimitResult = await enforceTtsRateLimit(env, request, user, ttsLimits, requestId);\n  if (rateLimitResult?.limited) {\n    const errorCode = rateLimitResult.tierLimited ? 'TIER_LIMIT' : 'RATE_LIMIT';\n    const errorMessage = rateLimitResult.tierLimited\n      ? `You've reached your monthly TTS limit (${ttsLimits.monthly}).`\n      : 'Too many speech token requests. Please wait a few moments and try again.';\n\n    return jsonResponse(\n      {\n        error: errorMessage,\n        errorCode,\n        tierLimited: rateLimitResult.tierLimited || false,\n        currentTier: subscription.tier,\n        limit: rateLimitResult.limit ?? null,\n        used: rateLimitResult.used ?? null,\n        resetAt: rateLimitResult.resetAt ?? null\n      },\n      {\n        status: 429,\n        headers: {\n          'retry-after': rateLimitResult.retryAfter.toString()\n        }\n      }\n    );\n  }\n\n  const speechKey = resolveEnv(env, 'AZURE_SPEECH_KEY');\n  const speechRegion = resolveEnv(env, 'AZURE_SPEECH_REGION') || 'eastus2';\n  const speechEndpoint = resolveEnv(env, 'AZURE_SPEECH_ENDPOINT');\n\n  if (!speechKey) {\n    console.error('[SpeechToken] AZURE_SPEECH_KEY not configured');\n    return jsonResponse(\n      { error: 'Speech service not configured' },\n      { status: 503 }\n    );\n  }\n\n  try {\n    // Custom endpoints (Azure AI Foundry) use endpoint-based token URL\n    // Standard Speech Service uses region-based URL\n    let tokenEndpoint;\n    if (speechEndpoint) {\n      // Custom endpoint: append token path to base endpoint\n      const baseUrl = speechEndpoint.replace(/\\/$/, '');\n      tokenEndpoint = `${baseUrl}/sts/v1.0/issueToken`;\n    } else {\n      // Region-based: standard Cognitive Services token endpoint\n      tokenEndpoint = `https://${speechRegion}.api.cognitive.microsoft.com/sts/v1.0/issueToken`;\n    }\n\n    const tokenResponse = await fetch(tokenEndpoint, {\n      method: 'POST',\n      headers: {\n        'Ocp-Apim-Subscription-Key': speechKey,\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Content-Length': '0'\n      }\n    });\n\n    if (!tokenResponse.ok) {\n      console.error('[SpeechToken] Azure token fetch failed:', tokenResponse.status, tokenResponse.statusText);\n\n      if (isDebugEnabled(env)) {\n        const errorText = await tokenResponse.text().catch(() => '');\n        if (errorText) {\n          console.error('[SpeechToken] Error details:', errorText);\n        }\n      }\n\n      return jsonResponse(\n        { error: 'Failed to get speech token' },\n        { status: 502 }\n      );\n    }\n\n    const token = await tokenResponse.text();\n","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"functions__api__speech-token__part-1.mp3"}
{"input":"    return jsonResponse(\n      {\n        token,\n        region: speechRegion,\n        expiresIn: 540 // 9 minutes in seconds\n      },\n      {\n        headers: {\n          'cache-control': 'no-store, no-cache, must-revalidate',\n          pragma: 'no-cache'\n        }\n      }\n    );\n  } catch (err) {\n    console.error('[SpeechToken] Error:', err);\n    return jsonResponse(\n      { error: 'Token service unavailable' },\n      { status: 500 }\n    );\n  }\n}\n\nfunction isDebugEnabled(env) {\n  const explicit = resolveEnv(env, 'ENABLE_SPEECH_TOKEN_DEBUG');\n  if (explicit) {\n    const normalized = String(explicit).trim().toLowerCase();\n    return normalized === 'true' || normalized === '1' || normalized === 'yes';\n  }\n\n  const nodeEnv = resolveEnv(env, 'NODE_ENV');\n  if (nodeEnv && nodeEnv.toLowerCase() !== 'production') {\n    return true;\n  }\n\n  return false;\n}\n","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"functions__api__speech-token__part-2.mp3"}
{"input":"/**\n * Audio Cache for TTS responses\n * \n * Caches short audio clips (phrases under a certain length) in memory\n * to avoid repeated API calls for common utterances.\n */\n\nconst MAX_CACHE_SIZE = 50;\nconst MAX_CACHEABLE_LENGTH = 200; // Only cache short phrases\n\nclass AudioCache {\n  constructor() {\n    this.cache = new Map();\n  }\n\n  /**\n   * Generate a cache key from text and emotion\n   * @param {string} text - The text content\n   * @param {string} emotion - The emotion/style applied\n   * @returns {string} Cache key\n   */\n  _key(text, emotion) {\n    return `${text}::${emotion || 'default'}`;\n  }\n\n  /**\n   * Check if text is short enough to be cached\n   * @param {string} text - Text to check\n   * @returns {boolean} Whether the text can be cached\n   */\n  isCacheable(text) {\n    return typeof text === 'string' && text.length <= MAX_CACHEABLE_LENGTH;\n  }\n\n  /**\n   * Get cached audio for text\n   * @param {string} text - The text\n   * @param {string} emotion - The emotion applied\n   * @returns {string|null} Data URL of cached audio, or null\n   */\n  get(text, emotion) {\n    if (!this.isCacheable(text)) return null;\n    const key = this._key(text, emotion);\n    const entry = this.cache.get(key);\n    if (entry) {\n      // Move to end (LRU behavior)\n      this.cache.delete(key);\n      this.cache.set(key, entry);\n      return entry;\n    }\n    return null;\n  }\n\n  /**\n   * Store audio in cache\n   * @param {string} text - The text\n   * @param {string} emotion - The emotion applied\n   * @param {string} audioDataUrl - Base64 data URL of the audio\n   */\n  set(text, emotion, audioDataUrl) {\n    if (!this.isCacheable(text)) return;\n    const key = this._key(text, emotion);\n    \n    // Evict oldest if at capacity\n    if (this.cache.size >= MAX_CACHE_SIZE) {\n      const firstKey = this.cache.keys().next().value;\n      if (firstKey) this.cache.delete(firstKey);\n    }\n    \n    this.cache.set(key, audioDataUrl);\n  }\n\n  /**\n   * Clear all cached audio\n   */\n  clear() {\n    this.cache.clear();\n  }\n\n  /**\n   * Get current cache size\n   * @returns {number} Number of cached entries\n   */\n  get size() {\n    return this.cache.size;\n  }\n}\n\nexport const audioCache = new AudioCache();\n","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__lib__audioCache__part-1.mp3"}
{"input":"import { useState, useEffect, useCallback, useRef } from 'react';\nimport { normalizeReadingTextForTtsHighlight } from '../lib/formatting.js';\nimport {\n  speakText,\n  pauseTTS,\n  resumeTTS,\n  stopTTS,\n  subscribeToTTS,\n  getCurrentTTSState,\n  unlockAudio,\n  enqueueTTSChunk,\n  finalizeTTSStream,\n  resetTTSStream,\n  isTTSStreamActive,\n  swellAmbience\n} from '../lib/audio';\nimport {\n  speakWithHume,\n  stopHumeAudio,\n  resetGenerationId\n} from '../lib/audioHume';\nimport {\n  initSpeechSDK,\n  isSpeechSDKReady,\n  synthesizeWithSDK,\n  cleanup as cleanupSpeechSDK\n} from '../lib/audioSpeechSDK';\nimport { usePreferences } from '../contexts/PreferencesContext';\n\nexport function useAudioController() {\n  const { voiceOn, setVoiceOn, ttsProvider, ttsSpeed, ambienceOn } = usePreferences();\n  const [ttsState, setTtsState] = useState(() => getCurrentTTSState());\n  const [ttsAnnouncement, setTtsAnnouncement] = useState('');\n  const [voicePromptRequested, setVoicePromptRequested] = useState(false);\n  const showVoicePrompt = voicePromptRequested && !voiceOn;\n\n  // Track Hume audio state separately\n  const humeAudioRef = useRef(null);\n  const [humeState, setHumeState] = useState({ status: 'idle', error: null });\n  const [humeFallbackActive, setHumeFallbackActive] = useState(false);\n  const humeRequestRef = useRef(0);\n\n  // Track Speech SDK state separately\n  const sdkAudioRef = useRef(null);\n  const sdkObjectUrlRef = useRef(null);\n  const [sdkState, setSdkState] = useState({ status: 'idle', error: null });\n  const [wordBoundary, setWordBoundary] = useState(null);\n  const sdkInitializedRef = useRef(false);\n  const sdkRequestRef = useRef(0);\n\n  const releaseSdkAudio = useCallback(() => {\n    if (sdkAudioRef.current) {\n      try {\n        sdkAudioRef.current.pause();\n        sdkAudioRef.current.currentTime = 0;\n        sdkAudioRef.current.src = '';\n      } catch {\n        // ignore pause errors\n      }\n      sdkAudioRef.current = null;\n    }\n    if (sdkObjectUrlRef.current && typeof URL !== 'undefined') {\n      try {\n        URL.revokeObjectURL(sdkObjectUrlRef.current);\n      } catch {\n        // ignore revoke errors\n      }\n      sdkObjectUrlRef.current = null;\n    }\n  }, []);\n\n  // Subscribe to TTS state changes\n  useEffect(() => {\n    const unsubscribe = subscribeToTTS(state => {\n      setTtsState(state);\n\n      const isAnnounceContext =\n        state.context === 'full-reading' ||\n        state.context === 'card-reveal';\n\n      const baseMessage =\n        state.message ||\n        (state.status === 'completed'\n          ? (state.context === 'card-reveal'\n            ? 'Card narration finished.'\n            : 'Narration finished.')\n          : state.status === 'paused'\n            ? (state.context === 'card-reveal'\n              ? 'Card narration paused.'\n              : 'Narration paused.')\n            : state.status === 'playing'\n              ? (state.context === 'card-reveal'\n                ? 'Card narration playing.'\n                : 'Narration playing.')\n              : state.status === 'loading'\n                ? (state.context === 'card-reveal'\n                  ? 'Preparing card narration.'\n                  : 'Preparing narration.')\n                : state.status === 'stopped'\n                  ? 'Narration stopped.'\n                  : state.status === 'error'\n                    ? 'Narration unavailable.'\n                    : '');\n\n      const announcement = isAnnounceContext ? baseMessage : '';\n      setTtsAnnouncement(announcement);\n    });\n    return unsubscribe;\n  }, []);\n\n  // Initialize Speech SDK when needed\n  useEffect(() => {\n    let cancelled = false;\n\n    if (ttsProvider === 'azure-sdk' && voiceOn && !sdkInitializedRef.current) {\n      initSpeechSDK({ voice: 'verse', enableSentenceBoundary: true })\n        .then(success => {\n          if (!cancelled && success) {\n            sdkInitializedRef.current = true;","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__hooks__useAudioController__part-1.mp3"}
{"input":"            console.log('[AudioController] Speech SDK initialized');\n          }\n        })\n        .catch(err => {\n          if (!cancelled) {\n            console.warn('[AudioController] Speech SDK init failed:', err);\n            setSdkState({ status: 'error', error: err?.message || 'Speech SDK unavailable' });\n          }\n        });\n    }\n\n    return () => {\n      cancelled = true;\n      if (ttsProvider === 'azure-sdk') {\n        releaseSdkAudio();\n        setSdkState({ status: 'idle', error: null });\n        setWordBoundary(null);\n        sdkInitializedRef.current = false;\n        void cleanupSpeechSDK();\n      }\n    };\n  }, [ttsProvider, voiceOn, releaseSdkAudio]);\n\n  // Stop TTS when voice is toggled off\n  useEffect(() => {\n    if (!voiceOn) {\n      humeRequestRef.current += 1;\n      sdkRequestRef.current += 1;\n      stopTTS();\n      stopHumeAudio();\n      setHumeFallbackActive(false);\n      setTimeout(() => {\n        setHumeState({ status: 'idle', error: null });\n      }, 0);\n      releaseSdkAudio();\n      setSdkState({ status: 'idle', error: null });\n      setWordBoundary(null);\n      if (sdkInitializedRef.current) {\n        sdkInitializedRef.current = false;\n        void cleanupSpeechSDK();\n      }\n    }\n  }, [voiceOn, releaseSdkAudio]);\n\n  useEffect(() => {\n    if (ttsProvider !== 'hume') {\n      setHumeFallbackActive(false);\n    }\n  }, [ttsProvider]);\n\n  // Azure TTS speak function with emotion and speed support\n  const speakWithAzure = useCallback(async (text, context = 'default', emotion = null) => {\n    await speakText({\n      text,\n      enabled: voiceOn,\n      context,\n      voice: 'nova', // Default voice for mystical tarot readings\n      speed: ttsSpeed,\n      emotion\n    });\n  }, [voiceOn, ttsSpeed]);\n\n  // Hume TTS speak function with emotion support\n  const speakWithHumeProvider = useCallback(async (text, context = 'default', emotion = null) => {\n    if (!text || !voiceOn) return;\n    const requestId = ++humeRequestRef.current;\n    const isStaleRequest = () => humeRequestRef.current !== requestId;\n\n    try {\n      setHumeFallbackActive(false);\n      // Stop any currently playing Hume audio\n      if (humeAudioRef.current) {\n        humeAudioRef.current.stop();\n      }\n\n      setHumeState({ status: 'loading', error: null });\n      setTtsAnnouncement('Preparing mystical narration...');\n\n      const result = await speakWithHume(text, {\n        context,\n        voiceName: 'ITO', // Warm, contemplative voice for readings\n        speed: 0.95, // Slightly slower for contemplation\n        emotion // GraphRAG-derived emotion for acting instructions\n      });\n\n      if (isStaleRequest()) {\n        result?.stop?.();\n        return;\n      }\n\n      humeAudioRef.current = result;\n      setHumeState({ status: 'playing', error: null });\n      setTtsAnnouncement('Playing your personalized reading...');\n\n      // Play the audio\n      await result.play();\n      if (isStaleRequest()) {\n        result?.stop?.();\n        return;\n      }\n\n      // Handle audio end\n      result.audio.onended = () => {\n        if (isStaleRequest()) return;\n        setHumeState({ status: 'completed', error: null });\n        setTtsAnnouncement('Narration finished.');\n        humeAudioRef.current = null;\n      };\n\n      // Handle audio errors\n      result.audio.onerror = (e) => {\n        if (isStaleRequest()) return;\n        console.error('Hume audio playback error:', e);\n        setHumeState({ status: 'error', error: 'Audio playback failed' });\n        setTtsAnnouncement('Narration unavailable.');\n        humeAudioRef.current = null;\n      };\n\n    } catch (error) {\n      if (isStaleRequest()) return;\n      console.error('Hume TTS error:', error);\n      setHumeState({ status: 'error', error: error.message || 'Failed to generate speech' });\n      setTtsAnnouncement('Hume unavailable. Falling back to Azure...');\n      setHumeFallbackActive(true);","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__hooks__useAudioController__part-2.mp3"}
{"input":"      await speakWithAzure(text, context, emotion);\n    }\n  }, [voiceOn, speakWithAzure]);\n\n  const enqueueNarrationChunk = useCallback((text, context = 'full-reading', emotion = null) => {\n    if (!voiceOn || !text) return false;\n    if (ttsProvider !== 'azure') return false;\n    const status = ttsState.status;\n    const isBusy = status === 'playing' || status === 'paused' || status === 'loading' || status === 'synthesizing';\n    if (isBusy && !isTTSStreamActive()) return false;\n\n    return enqueueTTSChunk({\n      text,\n      context,\n      voice: 'nova',\n      speed: ttsSpeed,\n      emotion\n    });\n  }, [voiceOn, ttsProvider, ttsSpeed, ttsState.status]);\n\n  const finalizeNarrationStream = useCallback(() => {\n    finalizeTTSStream();\n  }, []);\n\n  const resetNarrationStream = useCallback((options = {}) => {\n    resetTTSStream(options);\n  }, []);\n\n  const isNarrationStreamActive = useCallback(() => isTTSStreamActive(), []);\n\n  const pauseNarrationPlayback = useCallback(() => {\n    if (ttsProvider === 'hume' && humeAudioRef.current) {\n      try {\n        humeAudioRef.current.pause();\n      } catch {\n        // ignore pause errors\n      }\n      setHumeState({ status: 'paused', error: null });\n      setTtsAnnouncement('Narration paused.');\n      return;\n    }\n\n    if (ttsProvider === 'azure-sdk' && sdkAudioRef.current) {\n      try {\n        sdkAudioRef.current.pause();\n      } catch {\n        // ignore pause errors\n      }\n      setSdkState({ status: 'paused', error: null });\n      setTtsAnnouncement('Narration paused.');\n      return;\n    }\n\n    pauseTTS();\n  }, [ttsProvider]);\n\n  const resumeNarrationPlayback = useCallback(async () => {\n    if (ttsProvider === 'hume' && humeAudioRef.current?.audio) {\n      try {\n        await humeAudioRef.current.audio.play();\n        setHumeState({ status: 'playing', error: null });\n        setTtsAnnouncement('Resuming narration...');\n      } catch {\n        // ignore resume errors\n      }\n      return;\n    }\n\n    if (ttsProvider === 'azure-sdk' && sdkAudioRef.current) {\n      try {\n        await sdkAudioRef.current.play();\n        setSdkState({ status: 'playing', error: null });\n        setTtsAnnouncement('Resuming narration...');\n      } catch {\n        // ignore resume errors\n      }\n      return;\n    }\n\n    await resumeTTS();\n  }, [ttsProvider]);\n\n  // Azure Speech SDK with word-boundary events\n  const speakWithSpeechSDK = useCallback(async (text, context = 'default') => {\n    if (!text || !voiceOn) return;\n    const requestId = ++sdkRequestRef.current;\n    const isStaleRequest = () => sdkRequestRef.current !== requestId;\n    const normalizedText = normalizeReadingTextForTtsHighlight(text);\n\n    try {\n      if (!isSpeechSDKReady()) {\n        const success = await initSpeechSDK({ voice: 'verse', enableSentenceBoundary: true });\n        if (!success) {\n          throw new Error('Failed to initialize Speech SDK');\n        }\n        sdkInitializedRef.current = true;\n      }\n\n      releaseSdkAudio();\n      setSdkState({ status: 'loading', error: null });\n      setTtsAnnouncement('Preparing narration with word tracking...');\n      setWordBoundary(null);\n\n      const wordTimings = [];\n\n      const result = await synthesizeWithSDK(normalizedText, {\n        voice: 'verse',\n        context,\n        speed: 0.95,\n        enableViseme: false,\n        onStart: () => {\n          if (isStaleRequest()) return;\n          setSdkState({ status: 'synthesizing', error: null });\n        },\n        onWordBoundary: (event) => {\n          if (isStaleRequest()) return;\n          wordTimings.push(event);\n          if (event.boundaryType === 'word') {\n            setWordBoundary(event);\n          }\n        },\n        onComplete: () => {\n          if (isStaleRequest()) return;\n          setSdkState({ status: 'ready', error: null });\n        },\n        onError: (err) => {\n          if (isStaleRequest()) return;","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__hooks__useAudioController__part-3.mp3"}
{"input":"          console.error('[SpeechSDK] Synthesis error:', err);\n          setSdkState({ status: 'error', error: err?.errorDetails || err?.reason || 'Synthesis failed' });\n          setTtsAnnouncement('Narration unavailable.');\n        }\n      });\n\n      if (isStaleRequest()) {\n        if (result?.audioUrl && typeof URL !== 'undefined') {\n          try {\n            URL.revokeObjectURL(result.audioUrl);\n          } catch {\n            // ignore revoke errors\n          }\n        }\n        return;\n      }\n\n      const audio = new Audio(result.audioUrl);\n      sdkAudioRef.current = audio;\n      sdkObjectUrlRef.current = result.audioUrl;\n\n      audio.ontimeupdate = () => {\n        if (isStaleRequest()) return;\n        const currentMs = audio.currentTime * 1000;\n        const currentWord = wordTimings.find((w, index) => {\n          const next = wordTimings[index + 1];\n          return currentMs >= w.audioOffsetMs && (!next || currentMs < next.audioOffsetMs);\n        });\n        if (currentWord && currentWord.boundaryType === 'word') {\n          setWordBoundary(currentWord);\n        }\n      };\n\n      audio.onplay = () => {\n        if (isStaleRequest()) return;\n        setSdkState({ status: 'playing', error: null });\n        setTtsAnnouncement('Playing narration with word highlighting.');\n      };\n\n      audio.onpause = () => {\n        if (isStaleRequest()) return;\n        if (!audio.ended) {\n          setSdkState({ status: 'paused', error: null });\n          setTtsAnnouncement('Narration paused.');\n        }\n      };\n\n      audio.onended = () => {\n        if (isStaleRequest()) return;\n        releaseSdkAudio();\n        setSdkState({ status: 'completed', error: null });\n        setWordBoundary(null);\n        setTtsAnnouncement('Narration finished.');\n      };\n\n      audio.onerror = () => {\n        if (isStaleRequest()) return;\n        releaseSdkAudio();\n        setSdkState({ status: 'error', error: 'Audio playback failed' });\n        setWordBoundary(null);\n        setTtsAnnouncement('Narration unavailable.');\n      };\n\n      await audio.play();\n      if (isStaleRequest()) {\n        return;\n      }\n    } catch (error) {\n      if (isStaleRequest()) return;\n      releaseSdkAudio();\n      setWordBoundary(null);\n\n      const autoplayError = error?.name === 'NotAllowedError' ||\n        error?.message?.toLowerCase?.().includes('user interaction') ||\n        error?.message?.toLowerCase?.().includes('autoplay');\n\n      setSdkState({\n        status: 'error',\n        error: error?.message || 'Failed to generate speech'\n      });\n      setTtsAnnouncement(\n        autoplayError\n          ? 'Tap anywhere on the page to enable audio, then try again.'\n          : 'Narration unavailable.'\n      );\n\n      console.log('[AudioController] Falling back to Azure REST API');\n      await speakWithAzure(text, context);\n    }\n  }, [voiceOn, releaseSdkAudio, speakWithAzure]);\n\n  // Unified speak function that routes to appropriate provider\n  const speak = useCallback(async (text, context = 'default', emotion = null) => {\n    if (ttsProvider === 'hume') {\n      await speakWithHumeProvider(text, context, emotion);\n    } else if (ttsProvider === 'azure-sdk') {\n      await speakWithSpeechSDK(text, context);\n    } else {\n      // Azure TTS now supports emotion parameter via steerable instructions\n      await speakWithAzure(text, context, emotion);\n    }\n  }, [ttsProvider, speakWithHumeProvider, speakWithSpeechSDK, speakWithAzure]);\n\n  const swellCooldownRef = useRef(0);\n  const triggerCinematicSwell = useCallback((beatKey) => {\n    if (!ambienceOn) return;\n    const now = Date.now();\n    if (now - swellCooldownRef.current < 1800) return;\n    swellCooldownRef.current = now;\n\n    const beatPeakMap = {\n      opening: 0.28,\n      cards: 0.3,\n      pivot: 0.34,\n      tension: 0.32,\n      synthesis: 0.36,\n      resolution: 0.34,\n      guidance: 0.33\n    };","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__hooks__useAudioController__part-4.mp3"}
{"input":"    const peak = beatPeakMap[beatKey] || 0.3;\n    swellAmbience({ peak });\n  }, [ambienceOn]);\n\n  const handleNarrationButtonClick = useCallback(async (fullReadingText, isPersonalReadingError, emotion = null) => {\n    if (!voiceOn) {\n      setVoicePromptRequested(true);\n      return;\n    }\n    const isNarrationAvailable = Boolean(fullReadingText);\n    if (!isNarrationAvailable || isPersonalReadingError) return;\n\n    const shouldUseAzureFallback = ttsProvider === 'hume' && humeFallbackActive;\n    // Determine current state based on provider\n    const currentState = shouldUseAzureFallback\n      ? ttsState\n      : ttsProvider === 'hume'\n        ? humeState\n        : ttsProvider === 'azure-sdk'\n          ? sdkState\n          : ttsState;\n    const isLoading = currentState.status === 'loading' || currentState.status === 'synthesizing';\n\n    // Check if any provider is playing to prevent overlap\n    const isHumePlaying = humeState.status === 'playing';\n    const isAzurePlaying = ttsState.status === 'playing';\n    const isSdkPlaying = sdkState.status === 'playing';\n    const isPlaying = isHumePlaying || isAzurePlaying || isSdkPlaying;\n    const isPaused = currentState.status === 'paused';\n\n    if (isLoading && !isPaused && !isPlaying) return;\n\n    if (isPlaying) {\n      if (isHumePlaying && humeAudioRef.current) {\n        humeAudioRef.current.pause();\n        setHumeState({ status: 'paused', error: null });\n        setTtsAnnouncement('Narration paused.');\n      }\n\n      if (isSdkPlaying && sdkAudioRef.current) {\n        sdkAudioRef.current.pause();\n        setSdkState({ status: 'paused', error: null });\n        setTtsAnnouncement('Narration paused.');\n      }\n\n      if (isAzurePlaying) {\n        pauseTTS();\n      }\n      return;\n    }\n\n    const unlocked = await unlockAudio();\n    if (!unlocked) {\n      return;\n    }\n\n    if (isPaused) {\n      if (ttsProvider === 'hume' && humeAudioRef.current) {\n        humeAudioRef.current.audio.play();\n        setHumeState({ status: 'playing', error: null });\n        setTtsAnnouncement('Resuming narration...');\n      } else if (ttsProvider === 'azure-sdk' && sdkAudioRef.current) {\n        void sdkAudioRef.current.play();\n        setSdkState({ status: 'playing', error: null });\n        setTtsAnnouncement('Resuming narration...');\n      } else {\n        void resumeTTS();\n      }\n      return;\n    }\n\n    // Reset voice continuity when starting a new reading narration\n    if (ttsProvider === 'hume') {\n      resetGenerationId();\n    }\n\n    void speak(fullReadingText, 'full-reading', emotion);\n  }, [voiceOn, ttsState, humeState, sdkState, ttsProvider, humeFallbackActive, speak]);\n\n  const handleNarrationStop = useCallback(() => {\n    humeRequestRef.current += 1;\n    sdkRequestRef.current += 1;\n    // Always attempt to stop both providers to prevent orphaned audio\n    stopHumeAudio();\n    if (humeAudioRef.current) {\n      humeAudioRef.current = null;\n    }\n    setHumeState({ status: 'stopped', error: null });\n    setHumeFallbackActive(false);\n    resetGenerationId(); // Reset for next reading\n\n    releaseSdkAudio();\n    setSdkState({ status: 'stopped', error: null });\n    setWordBoundary(null);\n\n    stopTTS();\n    setTtsAnnouncement('Narration stopped.');\n  }, [releaseSdkAudio]);\n\n  const handleVoicePromptEnable = useCallback(async (fullReadingText, emotion) => {\n    setVoiceOn(true);\n    setVoicePromptRequested(false);\n    if (!fullReadingText) return;\n    const unlocked = await unlockAudio();\n    if (!unlocked) return;\n    setTimeout(() => {\n      void speak(fullReadingText, 'full-reading', emotion);\n    }, 120);\n  }, [setVoiceOn, speak]);\n\n  const setShowVoicePrompt = useCallback((nextVisible) => {\n    setVoicePromptRequested(Boolean(nextVisible));\n  }, []);\n\n  // Expose the correct TTS state based on current provider\n  let effectiveTtsState;\n  if (ttsProvider === 'hume' && humeFallbackActive) {","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__hooks__useAudioController__part-5.mp3"}
{"input":"    effectiveTtsState = ttsState;\n  } else if (ttsProvider === 'hume') {\n    effectiveTtsState = humeState;\n  } else if (ttsProvider === 'azure-sdk') {\n    effectiveTtsState = sdkState;\n  } else {\n    effectiveTtsState = ttsState;\n  }\n\n  return {\n    ttsState: effectiveTtsState,\n    ttsAnnouncement,\n    showVoicePrompt,\n    setShowVoicePrompt,\n    speak,\n    enqueueNarrationChunk,\n    finalizeNarrationStream,\n    resetNarrationStream,\n    isNarrationStreamActive,\n    pauseNarrationPlayback,\n    resumeNarrationPlayback,\n    handleNarrationButtonClick,\n    handleNarrationStop,\n    handleVoicePromptEnable,\n    ttsProvider,\n    wordBoundary,\n    triggerCinematicSwell\n  };\n}\n","voice":"cedar","instructions":"Voice Affect: Clear and composed. Tone: Neutral and technical. Pacing: Steady and moderate. Pronunciation: Enunciate file paths and common acronyms clearly. Pauses: Brief pauses at line breaks. Delivery: Consistent and readable for developer listening.","response_format":"mp3","out":"src__hooks__useAudioController__part-6.mp3"}
