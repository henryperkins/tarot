Streaming quality/heuristic tweaks look cohesive, but I spotted two regressions that risk masking when a stream is rejected or letting hallucinated readings slip past the last safety net.

- **High** `functions/api/tarot-reading.js:906-978` – when the new quality gate rejects Azure streaming output (qualityIssues > 0) we set `streamingFallback = true`, skip the safety scan, and fall through to buffered backends without returning any SSE response for the original stream. The final SSE now comes from the fallback backend, so there is no `gateBlocked`/`gateReason` metadata to tell the client that the first stream was discarded for quality reasons; telemetry/UI only sees a normal reading even though the stream was blocked. Can we mark the meta/done events (or backendErrors) with a quality-gate reason and keep the “gateBlocked” flag so clients can surface that fallback?

- **Medium** `functions/lib/evaluation.js:1389-1508` (plus `tests/evaluation.test.mjs:232-250`) – the heuristic fallback now leaves `scores.safety_flag` false whenever the number of hallucinated cards is at or below `maxHallucinations` (which defaults to 1 for small spreads). `checkEvalGate` blocks only when `safety_flag` true (or `safety`/`tone` are low), so a single hallucinated card now quietly bypasses the evaluation gate even though the quality metrics still report it. Previously every hallucination forced `safety_flag = true`, so this is a behavioral regression unless this relaxation is deliberate; we should double-check that a tolerated hallucination is something we want to expose to the user without any gate metadata.

Tests: `npm test`
